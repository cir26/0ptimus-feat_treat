{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feat_treat() class"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This class streamlines the process of treating, sampling, tuning, and testing features.\n",
    "By allowing the user to create instances of feature engineered data sets, the user can treat the same data set in multiple ways and then test all developed treatments in an automated fashion, all under the same roof and with fewer lines of code. \n",
    "\n",
    "Once instantiated, each feature treatment object has the following attributes:\n",
    "        self.X            --> feature dataframe\n",
    "        self.y            --> target dataframe\n",
    "        self.random_state --> seed used throughout tests for psuedo random number generator\n",
    "        self.encoded      --> boolean used to identify whether original dataframe has been encoded for testing\n",
    "        self.na_col       --> identified columns with missing values\n",
    "        self.metrics      --> perfomance metrics dataframe\n",
    "\n",
    "The tedious process of splitting into training and test sets, sampling, hyperparameter tuning, and testing is automated within a method in this class refered to as \"sample_tune_test()\". Although \"sample_tune_test()\" is the main use case of this package, other functions have been included to bring together and simplify some of the best packages in the python universe for feature treatment. \n",
    "\n",
    "Currently works with RandomForestClassifier, XGBClassifier, and LogisticRegression.\n",
    "More model support to come in the near future."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sample_tune_test() Current Run Time for random search strategy: \n",
    "RandomForestClassifier = 4.5-6.0 iterations/min\n",
    "XGBClassifier = 5.2 iterations/min\n",
    "\n",
    "sample_tune_test() Current Run Time for bayes search strategy: \n",
    "RandomForestClassifier = 1.5 iterations/min\n",
    "XGBClassifier = 1.6 iterations/min"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Other useful methods within this class include:\n",
    "\n",
    "check_na()             -->> Use to identify missing values in column(s) based on a percent missing threshold.\n",
    "                            Reports back the column names and the percent of missing values for columns with missing \n",
    "                            values greater than the percent threshold.\n",
    "                            Updates \"na_col\" attribute with list of column names that have missing values greater than \n",
    "                            the percent threshold.\n",
    "               \n",
    "handle_na()            -->> Use to impute missing values, must specify imputation strategy and desired column(s).\n",
    "                            Includes functionality for imputing missing values with the median, mean, or mode of the \n",
    "                            column in question.\n",
    "                            Also accepts any value given whether it be numerical or string to impute the \n",
    "                            column(s) in question.\n",
    "                            Also can completely remove column(s) in question.\n",
    "                            \n",
    "encode()               -->> Use to dummy encode categorical columns.\n",
    "                            Calls upon pd.get_dummies to accomplish this task.\n",
    "                                                        \n",
    "pcc_filter()           -->> Use to filter features based on pearson correlation coefficient.\n",
    "                            Must specify correlation limit.\n",
    "                            \n",
    "collinear()            -->> Use to remove features that are collinear.\n",
    "                            Must specify correlation limit.\n",
    "                            \n",
    "rfe()                  -->> Use to implement recursive feature elimination.\n",
    "                            Calls upon RFE in sklearn.feature_selection.\n",
    "                            Must specify number of features to eliminate to.\n",
    "\n",
    "skb()                  -->> Use to select K best features based on a callable scoring function like 'chi2' or     \n",
    "                            'GenericUnivariateSelect'.\n",
    "                            Calls upon SelectKBest in sklearn.feature_selection.\n",
    "                            Must specify minimal number of components to select.\n",
    "                            \n",
    "pca()                  -->> Use to perform principal component analysis\n",
    "                            Calls upon PCA in sklearn.decomposition.\n",
    "                            Must specify number of components to decompose to.\n",
    "                            \n",
    "svd()                  -->> Use to perform truncated singular value decomposition.\n",
    "                            Calls upon SVD in sklearn.decomposition.\n",
    "                            Must specify number of components to decompose to.                           \n",
    "\n",
    "\n",
    "More functionality to come in the near future.."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Now, I will walk you through an example using the M6 kantar file data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of feat_treat()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "RUN the final cell in this notebook first to execute class definition."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Begin by importing data into pandas dataframe.\n",
    "Before passing data sets into feat_treat(), user must separate feature set (X) from target set (y).\n",
    "In this case the data was separated in R prior to python import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import data which was subset using R\n",
    "df_y6= pd.read_csv('/home/cristianromero/Documents/Datasets/kantar_6_targets.csv')\n",
    "df_x6= pd.read_csv('/home/cristianromero/Documents/Datasets/kantar_6_features.csv')\n",
    "\n",
    "# remove extra id columns (relic of exporting data from R)\n",
    "df_y6=df_y6.iloc[:,1]\n",
    "df_x6=df_x6.iloc[:,1:len(df_x6)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now, the user can create an instance to begin feature treatment and testing using the feat_treat() call.\n",
    "\n",
    "Arguments should be passed as follows:\n",
    ">> feat_treat(X, y, random_state)\n",
    "\n",
    "Where:\n",
    "X            --> Feature pandas dataframe\n",
    "y            --> Target pandas dataframe\n",
    "random_state --> Seed to be used in pseudo random number generator throughout methods\n",
    "\n",
    "When instance is created, feat_treat automatically prints out the index of columns containing 'object' pandas datatypes.\n",
    "Additionally, dependent variable class counts and base rate is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment1 = feat_treat(df_x6,df_y6,42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "feat_treat() includes some functionality to impute missing values with the check_na() and handle_na() methods.\n",
    "The main use case of feat_treat() is NOT to impute missing values so this is feature is a bit limited at the moment.\n",
    "\n",
    "We begin with check_na().\n",
    "\n",
    "Arguments should be passed as follows:\n",
    ">> check_na(percent_threshold)\n",
    "\n",
    "Where:\n",
    "percent_threshold --> Percent threshold used to identify missing values. \n",
    "                      Any column with percent of missing values GREATER THAN percent_threshold will be identified.\n",
    "                      Percent_threshold = 50 refers to 50%.\n",
    "                      Percent_threshold = 0.5 refers to 0.5%.\n",
    "                      If no percent_threshold is passed, default is 0 (i.e all mising values identified)\n",
    "\n",
    "When check_na() method is applied, feat_treat() prints identified column names and their respective percent of missing values.\n",
    "Additionally, identified column names are returned to attribute within instance called \"na_col\".\n",
    "In this case we can access these identified columns by executing the following:\n",
    ">> treatment1.na_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values\n",
    "treatment1.check_na(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return identified columns\n",
    "treatment1.na_col"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now, we can handle missing values using the handle_na() method.\n",
    "\n",
    "Arguments should be passed as follows:\n",
    ">> handle_na(strategy, columns)\n",
    "\n",
    "Where:\n",
    "columns  --> List of column names as string to apply strategy to.\n",
    "strategy --> Strategy to handle missing values.\n",
    "             Accepts 'mean', 'median', or 'mode' and fills specified columns w/ respective column mean, median, or mode.\n",
    "             Accepts any string or numeric to fill specified columns with.\n",
    "             Accepts 'remove' to completely remove specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \n",
    "print(treatment1.X.shape)\n",
    "treatment1.handle_na('remove',treatment1.na_col)\n",
    "print(treatment1.X.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now, all other missing values are imputed with column median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment1.check_na() # run with no argument to return all columns with missing values\n",
    "treatment1.handle_na('median',treatment1.na_col)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Double-check all missing values have been taken care of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment1.check_na()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we can encode by applying the 'encode()' method to our dataset. \n",
    "\n",
    "Arguments should be passed as follows:\n",
    ">> encode(strategy)\n",
    "\n",
    "Where:\n",
    "strategy --> string\n",
    "             Strategy to encode categorical variables.\n",
    "             Accepts \"dummy\" to dummy encode (sparse encode) categorical columns using pd.get_dummies.\n",
    "             Accepts \"label\" to label encode categorical variables using LabelEncoder() from sklearn library.\n",
    "             Default is strategy=\"label\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment1.encode()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next import and specify the model of choice.\n",
    "It is IMPERATIVE the model is UNINSTANTIATED.\n",
    "\n",
    "RandomForestClassifier() --> Instantiated.\n",
    "RandomForestClassifier   --> UNINSTANTIATED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After features have been engineered/selected, we can use the sample_tune_test() method. This will automatically split into training/test sets, perform each sampling technique (RUS, ROS, SMOTE, SMOTE+TL), hyperparameter tune each model, test each model, and collect all performance metrics into a dataframe within the 'metrics' attribute of our feature treatment object.\n",
    "\n",
    "Arguments should be passed as follows:\n",
    ">> sample_tune_test(model, tuning_iter, tuning_strategy, tuning_metric, test_size)\n",
    "\n",
    "Where:\n",
    "model           --> estimator object\n",
    "                    UNINSTANTIATED model to be used throughout tests\n",
    "                    \n",
    "tuning_strategy --> string\n",
    "                    Pass \"randomized\" for randomized hyperparameter search.\n",
    "                    Pass \"bayes\" for bayesian hyperparameter optimization.\n",
    "                    Default is tuning_strategy = \"randomized\".\n",
    "                    \n",
    "tuning_iter     --> int\n",
    "                    Total number of hyperparameter tuning iterations to perform.\n",
    "                    \n",
    "tuning_metric   --> string\n",
    "                    Metric to use for tuning.\n",
    "                    Pass string of any scikit-learn metric like \"f1\", \"recall\", \"brier_score_loss\", etc.\n",
    "                    Default is tuning_metric = \"roc_auc\".\n",
    "                    \n",
    "test_size       --> float\n",
    "                    Specify fraction of data set tha is to become the test set. \n",
    "                    train_size = 1 - test_size.\n",
    "                    Default is test_size = 0.2        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment1.sample_tune_test(tuning_iter=50,model=classifier)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Performance metrics are stored in the metrics object attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment1.metrics"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Thus all the code we need to perform tests becomes as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier\n",
    "# impute\n",
    "treatmeant1=feat_treat(df_x6,df_y6,42)\n",
    "treatmeant1.check_na(50)\n",
    "treatmeant1.handle_na('remove',treatmeant1.na_col)\n",
    "treatmeant1.check_na()\n",
    "treatmeant1.handle_na('median',treatmeant1.na_col)\n",
    "# test\n",
    "treatmeant1.encode()\n",
    "treatment1.sample_tune_test(tuning_iter=100,model=classifier)\n",
    "treatmeant1.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature treatment class\n",
    "# Use to optimize feature selection, sampling, and hyperparameter tuning for model testing\n",
    "# Stream-lines testing process and brings together packages like scikit-learn, skopt, pandas, numpy \n",
    "# create instances of feature treatments \n",
    "# returns performance metrics for treatments with visual\n",
    "\n",
    "# Run Time: \n",
    "# RandomForestClassifier = 4.5 iterations/min\n",
    "# XGBClassifier = 5.2 iterations/min\n",
    "\n",
    "# required arguments: \n",
    "# X = feature pandas dataframe long format\n",
    "# y = dependent variable pandas dataframe long format\n",
    "# random_state = seed for pseudo random number generator to be used throughout treatments\n",
    "\n",
    "# main dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from math import floor, ceil, pi\n",
    "# scikit tools\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score,accuracy_score,roc_auc_score,brier_score_loss,confusion_matrix,f1_score,recall_score,precision_score,matthews_corrcoef\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "# sampling tools\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "class feat_treat:\n",
    "    def __init__(self,X,y,random_state):\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.random_state=random_state\n",
    "        self.encoded=False\n",
    "        self.na_col='update with check_na(percent_threshold)'\n",
    "        self.metrics='No metrics available'\n",
    "        self.model=\"No model available\" # pickle best model and store in attribute\n",
    "        \n",
    "        # Low variance filter \n",
    "        # check variance of each column\n",
    "        # remove column if variance is less than var_threshold\n",
    "        drop=[]\n",
    "        var_threshold = 0.004975\n",
    "        for col in self.X.columns:\n",
    "            if self.X[col].dtype!='object': \n",
    "                if np.var(self.X[col]) < var_threshold:\n",
    "                    drop.append(col)\n",
    "        self.X=self.X.drop(columns=drop)\n",
    "        \n",
    "        # GET TO KNOW YOUR DATA SET\n",
    "        # return dataframe indices for categorical datatypes\n",
    "        cat_col=[self.X.columns.get_loc(i) for i in self.X.columns if self.X[i].dtype == 'object']\n",
    "        print(\"categorical column indices: \",cat_col,\"\\n \")\n",
    "    \n",
    "        # return base rate\n",
    "        unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "        DV_classes_df = pd.DataFrame({'Class': unique_elements,\n",
    "                                      'Count': counts_elements})\n",
    "        print(DV_classes_df.to_string(index=False),\"\\n \")\n",
    "        print(\"Base rate: \",counts_elements[1]/sum(counts_elements))\n",
    "    \n",
    "\n",
    "    def check_na(self,percent_threshold=0):\n",
    "        # check for missing values  \n",
    "        percent_missing = self.X.isnull().sum() * 100 / len(self.X)\n",
    "        column_name = [percent_missing.index[i] for i in range(0,len(self.X.columns)) if percent_missing[i]>percent_threshold]\n",
    "        percent_na = [percent_missing[i] for i in range(0,len(self.X.columns)) if percent_missing[i]>percent_threshold]\n",
    "        missing_value_df = pd.DataFrame({'column_name': column_name,\n",
    "                                         'percent_na': percent_na})\n",
    "        missing_value_df.sort_values('percent_na', inplace=True)\n",
    "        print(missing_value_df, \"\\n \")\n",
    "        self.na_col=column_name\n",
    "    \n",
    "    \n",
    "    \n",
    "    def handle_na(self,strategy,columns):\n",
    "        # handle missing values\n",
    "        # methods: mean, median, mode, value, remove\n",
    "\n",
    "        if isinstance(strategy,int)==True or isinstance(strategy,float)==True:\n",
    "            for i in columns:\n",
    "                self.X[i]=self.X[i].fillna(strategy)\n",
    "        else:\n",
    "            strategy1=''.join(strategy.split()).lower()\n",
    "            if strategy1=='mean':\n",
    "                for i in columns:\n",
    "                    mean=self.X[i].mean()\n",
    "                    self.X[i].fillna(mean,inplace=True)\n",
    "            elif strategy1=='median':\n",
    "                for i in columns:\n",
    "                    median=self.X[i].median()\n",
    "                    self.X[i].fillna(median,inplace=True)                            \n",
    "            elif strategy1=='mode':\n",
    "                for i in columns:\n",
    "                    mode = self.X[i].mode()[0]\n",
    "                    self.X[i].fillna(mode,inplace=True)                      \n",
    "            elif strategy1=='remove':\n",
    "                self.X.drop(columns=self.na_col,inplace=True) \n",
    "            elif strategy1=='random':\n",
    "                for i in columns:\n",
    "                    uniques = np.unique(self.X[i])\n",
    "                    uniques = uniques[~np.isnan(uniques)]\n",
    "                    for j in range(0,len(self.X[i])):\n",
    "                        if np.isnan(self.X.loc[self.X.index[j],i])==True:\n",
    "                            self.X.loc[self.X.index[j],i] = np.random.choice(uniques)\n",
    "#             elif strategy1=='mice': # mice will perform impute on all columns\n",
    "#                 from impyute.imputation.cs import mice\n",
    "#                 col=self.X.columns\n",
    "#                 X_imp=mice(self.X)\n",
    "#                 self.X=pd.DataFrame(X_imp, columns=col)\n",
    "            else:\n",
    "                for i in columns:\n",
    "                    self.X[i].fillna(strategy,inplace=True)                 \n",
    "         \n",
    "    \n",
    "    \n",
    "    def pcc_filter(self,k):\n",
    "        if isinstance(k,int)==True or isinstance(k,float)==True:\n",
    "            # check pearson coefficient for linear correlation between DV and each feature \n",
    "            # remove column if PCC is less than correlation limit\n",
    "  \n",
    "            df=pd.concat([self.y, self.X], axis=1, ignore_index=True)\n",
    "#           create pearson correlation coefficient matrix\n",
    "            cor = df.corr()\n",
    "            corr_limit=k\n",
    "            cor_target = abs(cor.iloc[:,0])\n",
    "            relevant_features = cor_target[cor_target>corr_limit]\n",
    "            relevant_features=relevant_features.iloc[1:]\n",
    "            rel_col =[]\n",
    "            for i in relevant_features.index:\n",
    "                rel_col.append(i-1) \n",
    "#           drop all columns except relevant ones\n",
    "            self.X=self.X.iloc[:,rel_col]\n",
    "#           check which columns are kept \n",
    "            keep_col=self.X.columns\n",
    "            print(keep_col)\n",
    "            print(len(keep_col))            \n",
    "#       section to include pcc optimization loop\n",
    "        else:\n",
    "            k = ''.join(k.split()).lower()\n",
    "            k = k[:3]\n",
    "            if k == 'opt':\n",
    "                pass\n",
    "\n",
    "    \n",
    "    \n",
    "    def collinear(self,k):\n",
    "#       check pearson coefficient for linear correlation between features (collinearity)\n",
    "#       remove if PCC is greater than collinear limit\n",
    "#       recreate pearson correlation matrix\n",
    "        cor = self.X.corr()\n",
    "        feat_targets=[]\n",
    "        feat_remove=[]\n",
    "        ignore=[]\n",
    "#       set correlation limit\n",
    "        colinear_corr_limit=k\n",
    "#       check for collinearity\n",
    "        for i in self.X.columns:\n",
    "            cor_target = abs(cor.loc[:,i])\n",
    "            feat_targets.append((i,cor_target[cor_target>colinear_corr_limit]))  \n",
    "        # remove collinear features\n",
    "        for i in feat_targets:\n",
    "            ignore.append(i[0])\n",
    "            for j in i[1].index:\n",
    "                if j not in ignore:\n",
    "                    feat_remove.append(j)\n",
    "\n",
    "        feat_remove=set(feat_remove)\n",
    "        self.X=self.X.drop(columns=feat_remove)\n",
    "        # check which columns are kept \n",
    "        keep_col=self.X.columns\n",
    "        print(\"columns remaining: \",keep_col)\n",
    "        print(len(keep_col), \" columns\")\n",
    "                      \n",
    "    \n",
    "    def encode(self, strategy=None):\n",
    "        if(strategy=='dummy'):\n",
    "            self.X=pd.get_dummies(self.X)\n",
    "            self.encoded=True    \n",
    "        else:\n",
    "            cat_col=[i for i in self.X.columns if self.X[i].dtype == 'object']\n",
    "            if len(cat_col)>0:\n",
    "                le = preprocessing.LabelEncoder()\n",
    "                for col in cat_col:\n",
    "                    self.X[col]=le.fit_transform(self.X[col])            \n",
    "                self.encoded=True\n",
    "            else:\n",
    "                print(\"No categorical variables in data set\")\n",
    "        \n",
    "\n",
    "    def rfe(self,n=None,cum=None,rfe_model=None):\n",
    "        if isinstance(n,int)==True and cum==None:\n",
    "            if rfe_model==None:\n",
    "                from sklearn.linear_model import LogisticRegression\n",
    "                rfe_model=LogisticRegression(solver='lbfgs')\n",
    "                kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)\n",
    "                selector = RFECV(estimator=rfe_model, min_features_to_select = n, cv=kfold, n_jobs=-1).fit(self.X,self.y)\n",
    "                keep = [i for i in range(0,len(selector.support_)) if selector.support_[i]==True]\n",
    "                self.X=self.X.iloc[:,keep]\n",
    "            else:\n",
    "                kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)\n",
    "                selector = RFECV(estimator=rfe_model, min_features_to_select = n, cv=kfold, n_jobs=-1).fit(self.X,self.y)\n",
    "                keep = [i for i in range(0,len(selector.support_)) if selector.support_[i]==True]\n",
    "                self.X=self.iloc[:,keep]\n",
    "        elif isinstance(cum,float)==True and n==None:\n",
    "            # cumulative feature importance\n",
    "            pass\n",
    "#       section to include optimization loop\n",
    "        else:\n",
    "            n = ''.join(n.split()).lower()\n",
    "            n = n[:3]\n",
    "            if n == 'opt':\n",
    "                pass                \n",
    "\n",
    "            \n",
    "\n",
    "    def skb(self,k,score_func=None):\n",
    "        cat_col=[self.X.columns.get_loc(i) for i in self.X.columns if self.X[i].dtype == 'object']\n",
    "        if self.encoded == True or len(cat_col)==0:\n",
    "            if isinstance(k,int)==True or isinstance(k,float)==True:\n",
    "                if score_func==None:\n",
    "                    from sklearn.feature_selection import chi2\n",
    "                    skb =SelectKBest(score_func=chi2, k=k).fit(self.X,self.y)\n",
    "                    self.X = self.X.iloc[:,skb.get_support(indices=True)]\n",
    "                else:\n",
    "                    from sklearn.feature_selection import score_func\n",
    "                    skb = SelectKBest(score_func=score_func, k=k).fit(self.X,self.y)\n",
    "                    self.X = self.X.iloc[:,skb.get_support(indices=True)]\n",
    "#           section to include optimization loop\n",
    "            else:\n",
    "                n = ''.join(k.split()).lower()\n",
    "                n = n[:3]\n",
    "                if n == 'opt':\n",
    "                    pass              \n",
    "        else:\n",
    "            print('Please encode feature set first')\n",
    "            \n",
    "            \n",
    "            \n",
    "    def svd(self,n=None, var=None):\n",
    "        if var==None and isinstance(n,int)==True:\n",
    "            col = self.X.columns\n",
    "            svd = TruncatedSVD(n_components=n, n_iter=5, random_state=self.random_state).fit(self.X)\n",
    "            self.X = pd.DataFrame(svd.transform(self.X), columns=['SV %i' % i for i in range(n)], index=self.X.index)\n",
    "        elif n==None and isinstance(var,float)==True:\n",
    "            n=len(self.X.columns)\n",
    "            exp_var = 0\n",
    "            svd = TruncatedSVD(n_components=n, n_iter=5, random_state=self.random_state).fit(self.X) \n",
    "            j=0\n",
    "            exp_var=0\n",
    "            while exp_var<var and j<n:\n",
    "                exp_var = exp_var + svd.explained_variance_ratio_[j]\n",
    "                j=j+1\n",
    "            svd = TruncatedSVD(n_components=j, n_iter=5, random_state=self.random_state).fit(self.X) \n",
    "            self.X = pd.DataFrame(svd.transform(self.X), columns=['SV %i' % i for i in range(n)], index=self.X.index)\n",
    "#       section to include optimization loop\n",
    "        else:\n",
    "            n = ''.join(n.split()).lower()\n",
    "            n = n[:3]\n",
    "            if n == 'opt':\n",
    "                pass             \n",
    "            \n",
    "\n",
    "            \n",
    "    def pca(self,n=None,var=None):\n",
    "        if var==None and isinstance(n,int)==True:\n",
    "            pca = PCA(n_components=n, random_state=self.random_state).fit(self.X) \n",
    "            self.X = pd.DataFrame(pca.transform(self.X), columns=['PCA %i' % i for i in range(n)], index=self.X.index) \n",
    "        elif n==None and isinstance(var,float)==True:\n",
    "            n=len(self.X.columns)\n",
    "            exp_var = 0\n",
    "            pca = PCA(n_components=n, random_state=self.random_state).fit(self.X) \n",
    "            j=0\n",
    "            exp_var=0\n",
    "            while exp_var<var and j<n:\n",
    "                exp_var = exp_var + pca.explained_variance_ratio_[j]\n",
    "                j=j+1\n",
    "            pca = PCA(n_components=j, random_state=self.random_state).fit(self.X) \n",
    "            self.X = pd.DataFrame(pca.transform(self.X), columns=['PCA %i' % i for i in range(n)], index=self.X.index) \n",
    "#       section to include optimization loop\n",
    "        else:\n",
    "            n = ''.join(n.split()).lower()\n",
    "            n = n[:3]\n",
    "            if n == 'opt':\n",
    "                pass             \n",
    "\n",
    "    \n",
    "#   def sample(strategy):\n",
    "    def sample_tune_test(self,model,tuning_iter,tuning_strategy='randomized',tuning_metric='roc_auc',test_size=0.2):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=test_size, random_state=self.random_state)\n",
    "#       function to automatically sample, HP tune, then test with best parameters\n",
    "        col = X_train.columns\n",
    "        \n",
    "        rus = RandomUnderSampler()\n",
    "        X_rus, y_rus = rus.fit_sample(X_train, y_train)\n",
    "        X_rus = pd.DataFrame(X_rus, columns = col)\n",
    "\n",
    "        ros = RandomOverSampler()\n",
    "        X_ros, y_ros = ros.fit_sample(X_train, y_train)\n",
    "        X_ros = pd.DataFrame(X_ros, columns = col)\n",
    "        \n",
    "        smote = SMOTE(ratio='minority')\n",
    "        X_sm, y_sm = smote.fit_sample(X_train, y_train)\n",
    "        X_sm = pd.DataFrame(X_sm, columns = col)\n",
    "\n",
    "        smt = SMOTETomek(ratio='auto')\n",
    "        X_smt, y_smt = smt.fit_sample(X_train, y_train)\n",
    "        X_smt = pd.DataFrame(X_smt, columns = col)\n",
    "        \n",
    "        samples = ((X_train,y_train,\"None\"),\n",
    "                  (X_rus,y_rus,\"Random under sampling\"),\n",
    "                  (X_ros,y_ros,\"Random over sampling\"),\n",
    "                  (X_sm,y_sm,\"SMOTE\"),\n",
    "                  (X_smt,y_smt,\"SMOTE + TL\")) \n",
    "        N = len(samples)\n",
    "#       tune and test\n",
    "        best_param=[0]*N\n",
    "        self.metrics=pd.DataFrame(columns=[\"Sampling\",\n",
    "                                           \"Accuracy\",\n",
    "                                           'Precision',\n",
    "                                           'Recall',\n",
    "                                           'Specificity',\n",
    "                                           'Neg Pred Val',\n",
    "                                           'Confusion Sum',\n",
    "                                           'F1 score', \n",
    "                                           'F2 score',\n",
    "                                           'F3 score',\n",
    "                                           'Cohen kappa score',\n",
    "#                                              \"RMSE\" : rmse,\n",
    "#                                              \"Jaccard score\" : jaccard,\n",
    "                                           \"Brier score loss\",\n",
    "                                           'MCC',\n",
    "                                           \"AUC\"])\n",
    "        radar_df = pd.DataFrame(columns=['Sampling',\n",
    "                                            'F1', \n",
    "                                            'F2',\n",
    "                                            'F3',\n",
    "                                            'Cohen kappa',\n",
    "                                            'MCC',\n",
    "                                            'AUC'])\n",
    "        model_rep=model\n",
    "        print(\"Estimator: \",model_rep)\n",
    "        \n",
    "        for i in range(0,N):\n",
    "            model_inst=model_rep()\n",
    "            print(\"Sampling technique: \",samples[i][2], \"\\n \")\n",
    "#           IMPORTANT when inputting default hyperparameters:\n",
    "#           wrap non int or float types with Categorical() function\n",
    "#           NEVER input int or float types with only one parameter option... this will not work with bayes search (works fine with randomized search)\n",
    "            if(\"RandomForest\" in str(model_rep)):\n",
    "                tree_ensemble=True\n",
    "#               default hyperparameter testing range\n",
    "                bootstrap = Categorical([True, False])\n",
    "                n_estimators = [350,450]\n",
    "                criterion= Categorical(['gini','entropy'])\n",
    "                max_depth =np.arange(1,floor(len(X_train.columns)),1)\n",
    "                max_features = np.random.uniform(0.01,1,10000)\n",
    "                min_samples_split = np.random.uniform(0.01,1,10000)\n",
    "                min_samples_leaf = np.random.uniform(0.0001,0.5,10000)\n",
    "                class_weight = Categorical(['balanced','balanced_subsample',None])\n",
    "                n_jobs = [-1]\n",
    "                random_state = [self.random_state]\n",
    "#               input hyperparameters into dictionary\n",
    "                param_grid = dict(n_estimators=n_estimators,\n",
    "                                  bootstrap=bootstrap, \n",
    "                                  criterion=criterion,\n",
    "                                  min_samples_leaf=min_samples_leaf, \n",
    "                                  min_samples_split=min_samples_split,\n",
    "                                  max_features=max_features,\n",
    "                                  max_depth=max_depth,\n",
    "                                  class_weight=class_weight,\n",
    "                                  n_jobs=n_jobs,\n",
    "                                  random_state=random_state)\n",
    "\n",
    "            elif(\"XGB\" in str(model_rep)):\n",
    "                tree_ensemble=True\n",
    "    #               default hyperparameter testing range\n",
    "                booster = Categorical(['gbtree','gblinear','dart'])\n",
    "                n_estimators = [350,450]\n",
    "                learning_rate = np.random.uniform(0.000001,1,10000)\n",
    "                max_depth = np.arange(1,floor(len(X_train.columns)),1)\n",
    "                gamma = np.random.uniform(0,15,10000)\n",
    "                reg_alpha = np.random.uniform(0,1,10000)\n",
    "                reg_lambda = np.random.uniform(0,1,10000)\n",
    "                objective = Categorical(['reg:logistic'])\n",
    "                subsample = np.random.beta(2,5,10000)\n",
    "                colsample_bytree = np.random.beta(2,5,10000)\n",
    "                scale_pos_weight = np.random.uniform(0,20,10000)\n",
    "                min_child_weight = np.random.uniform(0,0.5*floor(len(X_train.columns)),10000)\n",
    "                max_delta_step = 0.2*floor(len(X_train.columns))*np.random.beta(2,5,10000)\n",
    "                n_jobs = [-1]\n",
    "                random_state = [self.random_state]\n",
    "    #               input hyperparameters into dictionary\n",
    "                param_grid = dict(n_estimators=n_estimators,\n",
    "                                  booster=booster,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  max_depth=max_depth,\n",
    "                                  gamma=gamma,\n",
    "                                  reg_alpha=reg_alpha,\n",
    "                                  reg_lambda=reg_lambda,\n",
    "                                  objective=objective,\n",
    "                                  subsample=subsample,\n",
    "                                  colsample_bytree=colsample_bytree,\n",
    "                                  scale_pos_weight=scale_pos_weight,\n",
    "                                  min_child_weight=min_child_weight,\n",
    "                                  max_delta_step=max_delta_step,\n",
    "                                  n_jobs=n_jobs,\n",
    "                                  random_state=random_state)          \n",
    "\n",
    "            elif(\"LogisticRegression\" in str(model_rep)):\n",
    "                tree_ensemble=False\n",
    "#               default hyperparameter testing range\n",
    "                penalty = Categorical(['l2'])\n",
    "                tol = np.random.uniform(0.00000001,0.01,10000)\n",
    "                C = np.random.uniform(0.000001,1,10000)\n",
    "                fit_intercept = Categorical([True,False])\n",
    "                intercept_scaling = np.random.uniform(0.01,10,10000)\n",
    "                class_weight = Categorical(['balanced',None])\n",
    "                solver = Categorical(['newton-cg', 'lbfgs','sag'])\n",
    "                max_iter = np.arange(100,1000,10)\n",
    "                n_jobs = [-1]\n",
    "                random_state = [self.random_state]\n",
    "#               input hyperparameters into dictionary\n",
    "                param_grid = dict(penalty=penalty,\n",
    "                                  tol=tol,\n",
    "                                  C=C,\n",
    "                                  fit_intercept=fit_intercept,\n",
    "                                  intercept_scaling=intercept_scaling,\n",
    "                                  class_weight=class_weight,\n",
    "                                  solver=solver,\n",
    "                                  max_iter=max_iter,\n",
    "                                  n_jobs=n_jobs,\n",
    "                                  random_state=random_state)\n",
    "\n",
    "            elif(\"Isolation\" in str(model_rep)):\n",
    "                tree_ensemble = True\n",
    "#               default hyperparameter testing range\n",
    "                bootstrap = Categorical([True, False])\n",
    "                n_estimators = [350,450]\n",
    "                max_samples= np.random.uniform(0.01,1,10000)\n",
    "                max_features = np.random.uniform(0.01,1,10000)\n",
    "                contamination = np.random.uniform(0,0.2,10000)\n",
    "                behaviour = Categorical(['new'])\n",
    "                n_jobs = [-1]\n",
    "                random_state = [self.random_state]\n",
    "#               input hyperparameters into dictionary\n",
    "                param_grid = dict(n_estimators=n_estimators,\n",
    "                                  bootstrap=bootstrap, \n",
    "                                  max_features=max_features,\n",
    "                                  contamination=contamination,\n",
    "                                  max_samples=max_samples,\n",
    "                                  behaviour=behaviour,\n",
    "                                  n_jobs=n_jobs,\n",
    "                                  random_state=random_state)\n",
    "\n",
    "            #elif(\"Bagging\" in str(model)):\n",
    "\n",
    "#           instantiate cv and grid\n",
    "            kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)\n",
    "            tuning_strategy=''.join(tuning_strategy.split()).lower()\n",
    "            if tuning_strategy=='bayes':\n",
    "                # unpack parameters\n",
    "                param_grid_cat = {k: v for k, v in param_grid.items() if isinstance(v,Categorical)==True or k=='n_jobs' or k=='random_state'}\n",
    "                param_grid_num = {k:(min(v),max(v)) for k,v in param_grid.items() if k not in param_grid_cat}\n",
    "                param_grid={**param_grid_num,**param_grid_cat}\n",
    "                import warnings\n",
    "                warnings.filterwarnings('ignore', message='The objective has been evaluated at this point before.')\n",
    "                grid_search = BayesSearchCV(model_inst, param_grid, scoring=tuning_metric, n_jobs=-1, n_points=4, cv=kfold, n_iter=floor(tuning_iter/N),verbose=0)\n",
    "            elif tuning_strategy=='randomized':\n",
    "                grid_search = RandomizedSearchCV(model_inst, param_grid, scoring=tuning_metric, n_jobs=-1, cv=kfold, n_iter=floor(tuning_iter/N),verbose=0)\n",
    "            print('Tuning...')\n",
    "            grid_results=grid_search.fit(samples[i][0],samples[i][1])\n",
    "            best_param[i]=grid_results.best_params_\n",
    "            print(\"Best {}: {} using {} \\n\".format(tuning_metric, grid_results.best_score_, best_param[i]))\n",
    "            if(tree_ensemble==True):\n",
    "                best_param[i].update({'n_estimators': 800})\n",
    "            del grid_results\n",
    "#           Train model\n",
    "            model=model_rep\n",
    "            model=model(**best_param[i])\n",
    "            print(\"Fitting model...\")\n",
    "            model.fit(samples[i][0],samples[i][1])\n",
    "            preds = model.predict(X_test[samples[i][0].columns])\n",
    "            conf_mat = confusion_matrix(y_true=y_test, y_pred=preds)\n",
    "            accuracy = accuracy_score(y_test, preds)\n",
    "            ck = cohen_kappa_score(y_test,preds)\n",
    "            brier = brier_score_loss(y_test,preds)\n",
    "            auc = roc_auc_score(y_test, preds) \n",
    "            f1 = f1_score(y_test, preds)\n",
    "#               jaccard = jaccard_score(y_test, preds)\n",
    "            recall = recall_score(y_test, preds)\n",
    "            precision = precision_score(y_test, preds)\n",
    "            mcc = matthews_corrcoef(y_test, preds)\n",
    "            specificity=round(conf_mat[0][0] / (conf_mat[0][0]+conf_mat[0][1]),5)\n",
    "            neg_pred= round(conf_mat[0][0] / (conf_mat[0][0]+conf_mat[1][0]),5)\n",
    "            f2=round(5*((precision*recall)/((4*precision)+recall)),5)\n",
    "            f3=round(2*((specificity*neg_pred)/(specificity+neg_pred)),5)\n",
    "#               rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "            print(\"Accuracy:          \", accuracy)\n",
    "            print('Precision:         ', precision)\n",
    "            print('Recall:            ', recall)\n",
    "            print('Specificity:       ', specificity)\n",
    "            print('Neg Pred Val:      ', neg_pred)\n",
    "            print('Confusion Sum:     ', precision+recall+specificity+neg_pred)                \n",
    "            print(' ')\n",
    "            print('F1 score:          ', f1)\n",
    "            print('F2 score:          ', f2)\n",
    "            print('F3 score:          ', f3)\n",
    "            print('Cohen kappa score: ', ck)\n",
    "#               print(\"RMSE:              \", rmse)\n",
    "            print(' ')\n",
    "#               print(\"Jaccard score:     \", jaccard)\n",
    "            print(\"Brier score loss:  \", brier)\n",
    "            print('MCC:               ', mcc)\n",
    "            print(\"AUC:               \", auc, \"\\n \",\"\\n \",\"\\n \",\"\\n \",\"\\n \") \n",
    "            df=pd.DataFrame({\"Sampling\" : samples[i][2],\n",
    "                                           \"Accuracy\" : round(accuracy,5),\n",
    "                                           'Precision' : round(precision,5),\n",
    "                                           'Recall' : round(recall,5),\n",
    "                                           'Specificity' : round(specificity,5),\n",
    "                                           'Neg Pred Val' : round(neg_pred,5),\n",
    "                                           'Confusion Sum' : round(precision+recall+specificity+neg_pred,5),\n",
    "                                           'F1 score' : round(f1,5),\n",
    "                                           'F2 score' : round(f2,5),\n",
    "                                           'F3 score' : round(f3,5),\n",
    "                                           'Cohen kappa score' : round(ck,5),\n",
    "#                                              \"RMSE\" : rmse,\n",
    "#                                              \"Jaccard score\" : jaccard,\n",
    "                                           \"Brier score loss\" : round(brier,5),\n",
    "                                           'MCC' : round(mcc,5),\n",
    "                                           \"AUC\" : round(auc,5)},index=[i])\n",
    "            df2 = pd.DataFrame({'Sampling': samples[i][2],\n",
    "                                            'F1': round(f1,5),\n",
    "                                            'F2': round(f2,5),\n",
    "                                            'F3': round(f3,5),\n",
    "                                            'Cohen kappa' : round(ck,5),\n",
    "                                            'MCC': round(mcc,5),\n",
    "                                            'AUC': round(auc,5)},index=[i])\n",
    "            self.metrics=self.metrics.append(df)\n",
    "            radar_df=radar_df.append(df2)\n",
    "            radar_df=radar_df.fillna(0)\n",
    "            \n",
    "        # ------- RADAR CHARTS PART 1: Create background\n",
    "        # number of variables\n",
    "        categories=list(radar_df)[1:]\n",
    "        N = len(categories)\n",
    "        # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "        angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "        angles += angles[:1]\n",
    "        # Initialise the spider plot\n",
    "        ax = plt.subplot(111, polar=True)\n",
    "        # If you want the first axis to be on top:\n",
    "        ax.set_theta_offset(pi / 2)\n",
    "        ax.set_theta_direction(-1)\n",
    "        # Draw one axe per variable + add labels labels yet\n",
    "        plt.xticks(angles[:-1], categories)\n",
    "        # Draw ylabels\n",
    "        ax.set_rlabel_position(0)\n",
    "        plt.yticks([0.2,0.4,0.6,0.8], [\"0.2\",\"0.4\",\"0.6\",\"0.8\"], color=\"blue\", size=7)\n",
    "        plt.ylim(0,1)\n",
    "        # ------- PART 2: Add plots\n",
    "        # Plot each individual = each line of the data\n",
    "        # Ind1\n",
    "        values=radar_df.loc[2].drop('Sampling').values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        values = [abs(number) for number in values]\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=\"ROS\")\n",
    "        ax.fill(angles, values, 'b', alpha=0.1)\n",
    "        # Ind2\n",
    "        # if statement to display smote or smote+TL based on AUC score\n",
    "        if radar_df.iloc[3,6] >= radar_df.iloc[4,6]:\n",
    "            values=radar_df.loc[3].drop('Sampling').values.flatten().tolist()\n",
    "            values += values[:1]\n",
    "            values = [abs(number) for number in values]\n",
    "            ax.plot(angles, values, linewidth=1, linestyle='solid', label=\"SMOTE\")\n",
    "        else:\n",
    "            values=radar_df.loc[4].drop('Sampling').values.flatten().tolist()\n",
    "            values += values[:1]\n",
    "            values = [abs(number) for number in values]\n",
    "            ax.plot(angles, values, linewidth=1, linestyle='solid', label=\"SMOTE + TL\")\n",
    "        ax.fill(angles, values, 'r', alpha=0.1)\n",
    "        # Ind3\n",
    "        values=radar_df.loc[1].drop('Sampling').values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        values = [abs(number) for number in values]\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=\"RUS\")\n",
    "        ax.fill(angles, values, 'k', alpha=0.1)\n",
    "        # Ind4\n",
    "        values=radar_df.loc[0].drop('Sampling').values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        values = [abs(number) for number in values]\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=\"No Sampling\")\n",
    "        ax.fill(angles, values, 'green', alpha=0.1)\n",
    "        # Add legend\n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
