{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feat_treat() class"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This class streamlines the process of treating, sampling, tuning, and testing features.\n",
    "By allowing the user to create instances of feature engineered data sets, the user can treat the same data set in multiple ways and then test all developed treatments in an automated fashion, all under the same roof and with fewer lines of code. \n",
    "\n",
    "Once instantiated, each feature treatment object has the following attributes:\n",
    "        self.X            --> feature dataframe\n",
    "        self.y            --> target dataframe\n",
    "        self.random_state --> seed used throughout tests for psuedo random number generator\n",
    "        self.encoded      --> boolean used to identify whether original dataframe has been encoded for testing\n",
    "        self.na_col       --> identified columns with missing values\n",
    "        self.metrics      --> perfomance metrics dataframe\n",
    "\n",
    "The tedious process of splitting into training and test sets, sampling, hyperparameter tuning, and testing is automated within a method in this class refered to as \"sample_tune_test()\". Although \"sample_tune_test()\" is the main use case of this package, other functions have been included to bring together and simplify some of the best packages in the python universe for feature treatment. \n",
    "\n",
    "Currently works with RandomForestClassifier, XGBClassifier, and LogisticRegression.\n",
    "More model support to come in the near future."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sample_tune_test() Current Run Time for random search strategy: \n",
    "RandomForestClassifier = 4.5-6.0 iterations/min\n",
    "XGBClassifier = 5.2 iterations/min\n",
    "\n",
    "sample_tune_test() Current Run Time for bayes search strategy: \n",
    "RandomForestClassifier =  iterations/min\n",
    "XGBClassifier = 1.6 iterations/min"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Other useful methods within this class include:\n",
    "\n",
    "check_na()             -->> Use to identify missing values in column(s) based on a percent missing threshold.\n",
    "                            Reports back the column names and the percent of missing values for columns with missing \n",
    "                            values greater than the percent threshold.\n",
    "                            Updates \"na_col\" attribute with list of column names that have missing values greater than \n",
    "                            the percent threshold.\n",
    "               \n",
    "handle_na()            -->> Use to impute missing values, must specify imputation strategy and desired column(s).\n",
    "                            Includes functionality for imputing missing values with the median, mean, or mode of the \n",
    "                            column in question.\n",
    "                            Also accepts any value given whether it be numerical or string to impute the \n",
    "                            column(s) in question.\n",
    "                            Also can completely remove column(s) in question.\n",
    "                            \n",
    "encode()               -->> Use to dummy encode categorical columns.\n",
    "                            Calls upon pd.get_dummies to accomplish this task.\n",
    "                                                        \n",
    "pcc_filter()           -->> Use to filter features based on pearson correlation coefficient.\n",
    "                            Must specify correlation limit.\n",
    "                            \n",
    "collinear()            -->> Use to remove features that are collinear.\n",
    "                            Must specify correlation limit.\n",
    "                            \n",
    "rfe()                  -->> Use to implement recursive feature elimination.\n",
    "                            Calls upon RFE in sklearn.feature_selection.\n",
    "                            Must specify number of features to eliminate to.\n",
    "                            \n",
    "pca()                  -->> Use to perform principal component analysis\n",
    "                            Calls upon PCA in sklearn.decomposition.\n",
    "                            Must specify number of components to decompose to.\n",
    "                            \n",
    "svd()                  -->> Use to perform truncated singular value decomposition.\n",
    "                            Calls upon SVD in sklearn.decomposition.\n",
    "                            Must specify number of components to decompose to.\n",
    "                            \n",
    "\n",
    "\n",
    "More functionality to come in the near future.."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Now, I will walk you through an example using the M6 kantar file data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of feat_treat()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "RUN the final cell in this notebook first to execute class definition."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Begin by importing data into pandas dataframe.\n",
    "Before passing data sets into feat_treat(), user must separate feature set (X) from target set (y).\n",
    "In this case the data was separated in R prior to python import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import data which was subset using R\n",
    "df_y6= pd.read_csv('/home/cristianromero/Documents/Datasets/kantar_6_targets.csv')\n",
    "df_x6= pd.read_csv('/home/cristianromero/Documents/Datasets/kantar_6_features.csv')\n",
    "\n",
    "# remove extra id columns (relic of exporting data from R)\n",
    "df_y6=df_y6.iloc[:,1]\n",
    "df_x6=df_x6.iloc[:,1:len(df_x6)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now, the user can create an instance to begin feature treatment and testing using the feat_treat() call.\n",
    "\n",
    "Arguments should be passed as follows:\n",
    ">> feat_treat(X, y, random_state)\n",
    "\n",
    "Where:\n",
    "X            --> Feature pandas dataframe\n",
    "y            --> Dependent variable pandas dataframe\n",
    "random_state --> Seed to be used in pseudo random number generator throughout methods\n",
    "\n",
    "When instance is created, feat_treat automatically prints out the index of columns containing 'object' pandas datatypes.\n",
    "Additionally, dependent variable class counts and base rate is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment1 = feat_treat(df_x6,df_y6,42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "feat_treat() includes some functionality to impute missing values with the check_na() and handle_na() methods.\n",
    "The main use case of feat_treat() is NOT to impute missing values so this is feature is a bit limited at the moment.\n",
    "\n",
    "We begin with check_na().\n",
    "\n",
    "Arguments should be passed as follows:\n",
    ">> check_na(percent_threshold)\n",
    "\n",
    "Where:\n",
    "percent_threshold --> Percent threshold used to identify missing values. \n",
    "                      Any column with percent of missing values GREATER THAN percent_threshold will be identified.\n",
    "                      Percent_threshold = 50 refers to 50%.\n",
    "                      Percent_threshold = 0.5 refers to 0.5%.\n",
    "                      If no percent_threshold is passed, default is 0 (i.e all mising values identified)\n",
    "\n",
    "When check_na() method is applied, feat_treat() prints identified column names and their respective percent of missing values.\n",
    "Additionally, identified column names are returned to attribute within instance called \"na_col\".\n",
    "In this case we can access these identified columns by executing the following:\n",
    ">> treatment1.na_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values\n",
    "treatment1.check_na(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return identified columns\n",
    "treatment1.na_col"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now, we can handle missing values using the handle_na() method.\n",
    "\n",
    "Arguments should be passed as follows:\n",
    ">> handle_na(strategy, columns)\n",
    "\n",
    "Where:\n",
    "columns  --> List of column names as string to apply strategy to.\n",
    "strategy --> Strategy to handle missing values.\n",
    "             Accepts 'mean', 'median', or 'mode' and fills specified columns w/ respective column mean, median, or mode.\n",
    "             Accepts any string or numeric to fill specified columns with.\n",
    "             Accepts 'remove' to completely remove specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \n",
    "treatment1.handle_na('remove',treatment1.na_col)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now, all other missing values are imputed with column median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment1.check_na() # run with no argument to return all columns with missing values\n",
    "treatment1.handle_na('median',treatment1.na_col)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Double-check all missing values have been taken care of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment1.check_na()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we can encode by applying the 'encode()' method to our dataset. \n",
    "\n",
    "Arguments should be passed as follows:\n",
    ">> encode(strategy)\n",
    "\n",
    "Where:\n",
    "strategy --> Strategy as string to encode categorical variables.\n",
    "             Accepts \"dummy\" to dummy encode (sparse encode) categorical columns using pd.get_dummies.\n",
    "             Accepts \"label\" to label encode categorical variables using LabelEncoder() from sklearn library.\n",
    "             Default is strategy=\"label\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment1.encode()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that the data has been prepped we can split into training and test sets by using the train_test_split() method.\n",
    "\n",
    "Arguments should be passed as follows:\n",
    ">> train_test_split(test_size)\n",
    "\n",
    "Where:\n",
    "\n",
    "\n",
    "After this split, features can be treated/engineered as desired, methods will automatically be applied to both training and test sets."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next import and specify the model of choice.\n",
    "It is IMPERATIVE the model is UNINSTANTIATED.\n",
    "\n",
    "RandomForestClassifier() --> Instantiated.\n",
    "RandomForestClassifier   --> UNINSTANTIATED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After features have been engineered/selected, we can use the sample_tune_test() method. This will automatically split into training/test sets, perform each sampling technique (RUS, ROS, SMOTE, SMOTE+TL), hyperparameter tune each model, test each model, and collect all performance metrics into a dataframe within the 'metrics' attribute of our feature treatment object.\n",
    "\n",
    "Arguments should be passed as follows:\n",
    ">> sample_tune_test(model, tuning_iter, tuning_strategy, tuning_metric, test_size)\n",
    "\n",
    "Where:\n",
    "model           --> object\n",
    "                    UNINSTANTIATED model to be used throughout tests\n",
    "                    \n",
    "tuning_strategy --> string\n",
    "                    Pass \"randomized\" for randomized hyperparameter search.\n",
    "                    Pass \"bayes\" for bayesian hyperparameter optimization.\n",
    "                    Default is tuning_strategy = \"randomized\".\n",
    "                    \n",
    "tuning_iter     --> int\n",
    "                    Total number of hyperparameter tuning iterations to perform.\n",
    "                    \n",
    "tuning_metric   --> string\n",
    "                    Metric to use for tuning.\n",
    "                    Pass string of any scikit-learn metric like \"f1\", \"recall\", \"brier_score_loss\", etc.\n",
    "                    Default is tuning_metric = \"roc_auc\".\n",
    "                    \n",
    "test_size       --> float\n",
    "                    Specify fraction of data set tha is to become the test set. \n",
    "                    train_size = 1 - test_size.\n",
    "                    Default is test_size = 0.2        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment1.sample_tune_test(tuning_iter=50,model=classifer)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Performance metrics are stored in the metrics object attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment1.metrics"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Thus all the code we need to perform tests becomes as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier\n",
    "# impute\n",
    "treatmeant1=feat_treat(df_x6,df_y6,42)\n",
    "treatmeant1.check_na(50)\n",
    "treatmeant1.handle_na('remove',treatmeant1.na_col)\n",
    "treatmeant1.check_na()\n",
    "treatmeant1.handle_na('median',treatmeant1.na_col)\n",
    "# test\n",
    "treatmeant1.encode()\n",
    "treatment1.sample_tune_test(tuning_iter=100,model=classifer)\n",
    "treatmeant1.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature treatment class\n",
    "# Use to optimize feature selection, sampling, and hyperparameter tuning for model testing\n",
    "# Stream-lines testing process and brings together packages like scikit-learn, skopt, pandas, numpy \n",
    "# create instances of feature treatments \n",
    "# returns performance metrics for treatments with visual\n",
    "\n",
    "# Run Time: \n",
    "# RandomForestClassifier = 4.5 iterations/min\n",
    "# XGBClassifier = 5.2 iterations/min\n",
    "\n",
    "# required arguments: \n",
    "# X = feature pandas dataframe long format\n",
    "# y = dependent variable pandas dataframe long format\n",
    "# random_state = seed for pseudo random number generator to be used throughout treatments\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class feat_treat:\n",
    "    def __init__(self,X,y,random_state):\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.random_state=random_state\n",
    "        self.encoded=False\n",
    "        self.na_col='update with check_na(percent_threshold)'\n",
    "        self.metrics='No metrics available'\n",
    "        \n",
    "        # Low variance filter \n",
    "        # check variance of each column\n",
    "        # remove column if variance is less than var_threshold\n",
    "        drop=[]\n",
    "        var_threshold = 0.004975\n",
    "        for col in self.X.columns:\n",
    "            if self.X[col].dtype!='object': \n",
    "                if np.var(self.X[col]) < var_threshold:\n",
    "                    drop.append(col)\n",
    "        self.X=self.X.drop(columns=drop)\n",
    "        \n",
    "        # GET TO KNOW YOUR DATA SET\n",
    "        # return dataframe indices for categorical datatypes\n",
    "        cat_col=[]\n",
    "        for i in self.X.columns:\n",
    "            if self.X[i].dtype == 'object':\n",
    "                cat_col.append(self.X.columns.get_loc(i))\n",
    "        print(\"categorical column indices: \",cat_col,\"\\n \")\n",
    "    \n",
    "        # return base rate\n",
    "        unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "        DV_classes_df = pd.DataFrame({'Class': unique_elements,\n",
    "                                      'Count': counts_elements})\n",
    "        print(DV_classes_df.to_string(index=False),\"\\n \")\n",
    "        print(\"Base rate: \",counts_elements[1]/sum(counts_elements))\n",
    "    \n",
    "    \n",
    "\n",
    "    def check_na(self,percent_threshold=0):\n",
    "        # check for missing values  \n",
    "        percent_missing = self.X.isnull().sum() * 100 / len(self.X)\n",
    "        column_name = [percent_missing.index[i] for i in range(0,len(self.X.columns)) if percent_missing[i]>0]\n",
    "        percent_na = [percent_missing[i] for i in range(0,len(self.X.columns)) if percent_missing[i]>0]\n",
    "        missing_value_df = pd.DataFrame({'column_name': column_name,\n",
    "                                         'percent_na': percent_na})\n",
    "        missing_value_df.sort_values('percent_na', inplace=True)\n",
    "        print(missing_value_df, \"\\n \")\n",
    "        self.na_col=column_name\n",
    "    \n",
    "    \n",
    "    \n",
    "    def handle_na(self,strategy,columns):\n",
    "        # handle missing values\n",
    "        # methods: mean, median, mode, value, remove\n",
    "        from scipy import stats\n",
    "        \n",
    "        if isinstance(strategy,int)==True or isinstance(strategy,float)==True:\n",
    "            for i in columns:\n",
    "                self.X[i]=self.X[i].fillna(strategy)\n",
    "        else:\n",
    "            strategy1=''.join(strategy.split()).lower()\n",
    "            if strategy1=='mean':\n",
    "                for i in columns:\n",
    "                    mean=self.X[i].mean()\n",
    "                    self.X[i].fillna(mean,inplace=True)\n",
    "            elif strategy1=='median':\n",
    "                for i in columns:\n",
    "                    median=self.X[i].median()\n",
    "                    self.X[i].fillna(median,inplace=True)                            \n",
    "            elif strategy1=='mode':\n",
    "                for i in columns:\n",
    "                    mode = self.X[i].mode()[0]\n",
    "                    self.X[i].fillna(mode,inplace=True)                      \n",
    "            elif strategy1=='remove':\n",
    "                self.X.drop(columns=self.na_col,inplace=True) \n",
    "            else:\n",
    "                for i in columns:\n",
    "                    self.X[i].fillna(strategy,inplace=True)                 \n",
    "         \n",
    "    \n",
    "    \n",
    "    def pcc_filter(self,k):\n",
    "        if isinstance(k,int)==True or isinstance(k,float)==True:\n",
    "            # check pearson coefficient for linear correlation between DV and each feature \n",
    "            # remove column if PCC is less than correlation limit\n",
    "  \n",
    "            df=pd.concat([self.y, self.X], axis=1, ignore_index=True)\n",
    "#           create pearson correlation coefficient matrix\n",
    "            cor = df.corr()\n",
    "            corr_limit=k\n",
    "            cor_target = abs(cor.iloc[:,0])\n",
    "            relevant_features = cor_target[cor_target>corr_limit]\n",
    "            relevant_features=relevant_features.iloc[1:]\n",
    "            rel_col =[]\n",
    "            for i in relevant_features.index:\n",
    "                rel_col.append(i-1) \n",
    "#           drop all columns except relevant ones\n",
    "            self.X=self.X.iloc[:,rel_col]\n",
    "#           check which columns are kept \n",
    "            keep_col=self.X.columns\n",
    "            print(keep_col)\n",
    "            print(len(keep_col))            \n",
    "#       section to include pcc optimization loop\n",
    "        else:\n",
    "            k = ''.join(k.split()).lower()\n",
    "            k = k[:3]\n",
    "            if k == 'opt':\n",
    "                pass\n",
    "\n",
    "    \n",
    "    \n",
    "    def collinear(self,k):\n",
    "#       check pearson coefficient for linear correlation between features (collinearity)\n",
    "#       remove if PCC is greater than collinear limit\n",
    "#       recreate pearson correlation matrix\n",
    "        cor = self.X.corr()\n",
    "        feat_targets=[]\n",
    "        feat_remove=[]\n",
    "        ignore=[]\n",
    "#       set correlation limit\n",
    "        colinear_corr_limit=k\n",
    "#       check for collinearity\n",
    "        for i in self.X.columns:\n",
    "            cor_target = abs(cor.loc[:,i])\n",
    "            feat_targets.append((i,cor_target[cor_target>colinear_corr_limit]))  \n",
    "        # remove collinear features\n",
    "        for i in feat_targets:\n",
    "            ignore.append(i[0])\n",
    "            for j in i[1].index:\n",
    "                if j not in ignore:\n",
    "                    feat_remove.append(j)\n",
    "\n",
    "        feat_remove=set(feat_remove)\n",
    "        self.X=self.X.drop(columns=feat_remove)\n",
    "        # check which columns are kept \n",
    "        keep_col=self.X.columns\n",
    "        print(\"columns remaining: \",keep_col)\n",
    "        print(len(keep_col), \" columns\")\n",
    "                      \n",
    "    \n",
    "    def encode(self, strategy=None):\n",
    "        if(strategy=='dummy'):\n",
    "            self.X=pd.get_dummies(self.X)\n",
    "            self.encoded=True    \n",
    "        else:\n",
    "            from sklearn import preprocessing\n",
    "            cat_col=[]\n",
    "            for i in self.X.columns:\n",
    "                if self.X[i].dtype == 'object':\n",
    "                    cat_col.append(i)\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            for col in cat_col:\n",
    "                self.X[col]=le.fit_transform(self.X[col])            \n",
    "            self.encoded=True    \n",
    "        \n",
    "    \n",
    "    \n",
    "    def rfe(self,n,rfe_model=None):\n",
    "        if isinstance(n,int)==True or isinstance(n,float)==True:\n",
    "            if rfe_model==None:\n",
    "                from sklearn.linear_model import LogisticRegression\n",
    "                rfe_model=LogisticRegression(solver='lbfgs')\n",
    "                from sklearn.feature_selection import RFE\n",
    "                selector = RFE(estimator=rfe_model, n_features_to_select = n).fit(self.X,self.y)\n",
    "                keep = [i for i in range(0,len(selector.support_)) if selector.support_[i]==True]\n",
    "                self.X=self.X.iloc[:,keep]\n",
    "            else:\n",
    "                from sklearn.feature_selection import RFE\n",
    "                selector = RFE(estimator=rfe_model, n_features_to_select = n).fit(self.X,self.y)\n",
    "                keep = [i for i in range(0,len(selector.support_)) if selector.support_[i]==True]\n",
    "                self.X=self.iloc[:,keep]\n",
    "#       section to include optimization loop\n",
    "        else:\n",
    "            n = ''.join(n.split()).lower()\n",
    "            n = n[:3]\n",
    "            if n == 'opt':\n",
    "                pass                \n",
    "\n",
    "            \n",
    "\n",
    "    def skb(self,k,score_func=None):\n",
    "        if self.encoded == True:\n",
    "            if isinstance(k,int)==True or isinstance(k,float)==True:\n",
    "                if score_func==None:\n",
    "                    from sklearn.feature_selection import SelectKBest, chi2\n",
    "                    skb =SelectKBest(score_func=chi2, k=k).fit(self.X,self.y)\n",
    "                    self.X = self.X.iloc[:,skb.get_support(indices=True)]\n",
    "                else:\n",
    "                    from sklearn.feature_selection import SelectKBest, score_func\n",
    "                    skb = SelectKBest(score_func=score_func, k=k).fit(self.X,self.y)\n",
    "                    self.X = self.X.iloc[:,skb.get_support(indices=True)]\n",
    "\n",
    "#           section to include optimization loop\n",
    "            else:\n",
    "                n = ''.join(k.split()).lower()\n",
    "                n = n[:3]\n",
    "                if n == 'opt':\n",
    "                    pass              \n",
    "        else:\n",
    "            print('Please encode feature set first')\n",
    "            \n",
    "            \n",
    "            \n",
    "    def svd(self,n):\n",
    "        if isinstance(n,int)==True or isinstance(n,float)==True:\n",
    "            col = self.X.columns\n",
    "            from sklearn.decomposition import TruncatedSVD\n",
    "            svd = TruncatedSVD(n_components=n, n_iter=5, random_state=self.random_state).fit(self.X)\n",
    "            self.X = pd.DataFrame(svd.transform(self.X), columns=['SV %i' % i for i in range(n)], index=self.X.index)  \n",
    "#       section to include optimization loop\n",
    "        else:\n",
    "            n = ''.join(n.split()).lower()\n",
    "            n = n[:3]\n",
    "            if n == 'opt':\n",
    "                pass             \n",
    "            \n",
    "\n",
    "            \n",
    "    def pca(self,n):\n",
    "        if isinstance(n,int)==True or isinstance(n,float)==True:\n",
    "            from sklearn.decomposition import PCA\n",
    "            pca = PCA(n_components=n, random_state=self.random_state).fit(self.X) \n",
    "            self.X = pd.DataFrame(pca.transform(self.X), columns=['PCA %i' % i for i in range(n)], index=self.X.index) \n",
    "#       section to include optimization loop\n",
    "        else:\n",
    "            n = ''.join(n.split()).lower()\n",
    "            n = n[:3]\n",
    "            if n == 'opt':\n",
    "                pass             \n",
    "\n",
    "    \n",
    "#   def sample(strategy):\n",
    "    def sample_tune_test(self,model,tuning_iter,tuning_strategy='randomized',tuning_metric='roc_auc',test_size=0.2):\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=test_size, random_state=self.random_state)\n",
    "#       function to automatically sample, HP tune, then test with best parameters\n",
    "        col = X_train.columns\n",
    "        from imblearn.under_sampling import RandomUnderSampler\n",
    "        rus = RandomUnderSampler()\n",
    "        X_rus, y_rus = rus.fit_sample(X_train, y_train)\n",
    "        X_rus = pd.DataFrame(X_rus, columns = col)\n",
    "        from imblearn.over_sampling import RandomOverSampler\n",
    "        ros = RandomOverSampler()\n",
    "        X_ros, y_ros = ros.fit_sample(X_train, y_train)\n",
    "        X_ros = pd.DataFrame(X_ros, columns = col)\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        smote = SMOTE(ratio='minority')\n",
    "        X_sm, y_sm = smote.fit_sample(X_train, y_train)\n",
    "        X_sm = pd.DataFrame(X_sm, columns = col)\n",
    "        from imblearn.combine import SMOTETomek\n",
    "        smt = SMOTETomek(ratio='auto')\n",
    "        X_smt, y_smt = smt.fit_sample(X_train, y_train)\n",
    "        X_smt = pd.DataFrame(X_smt, columns = col)\n",
    "        samples = [(X_train,y_train,\"None\"),\n",
    "                  (X_rus,y_rus,\"Random under sampling\"),\n",
    "                  (X_ros,y_ros,\"Random over sampling\"),\n",
    "                  (X_sm,y_sm,\"SMOTE\"),\n",
    "                  (X_smt,y_smt,\"SMOTE + TL\")] \n",
    "#       tune and test\n",
    "        best_param=[0]*len(samples)\n",
    "        self.metrics=pd.DataFrame(columns=[\"Sampling\",\n",
    "                                           \"Accuracy\",\n",
    "                                           'Precision',\n",
    "                                           'Recall',\n",
    "                                           'Specificity',\n",
    "                                           'Neg Pred Val',\n",
    "                                           'Confusion Sum',\n",
    "                                           'F1 score', \n",
    "                                           'F2 score',\n",
    "                                           'Cohen kappa score',\n",
    "#                                              \"RMSE\" : rmse,\n",
    "#                                              \"Jaccard score\" : jaccard,\n",
    "                                           \"Brier score loss\",\n",
    "                                           'MCC',\n",
    "                                           \"AUC\"])\n",
    "        radar_df = pd.DataFrame(columns=['Sampling',\n",
    "                                            'AUC',\n",
    "                                            'F1', \n",
    "                                            'F2',\n",
    "                                            'Cohen kappa',\n",
    "                                            'Brier',\n",
    "                                            'MCC'])\n",
    "        model_rep=model\n",
    "        print(\"Estimator: \",model_rep)\n",
    "        from sklearn.metrics import cohen_kappa_score,accuracy_score,roc_auc_score,brier_score_loss,confusion_matrix,f1_score,recall_score,precision_score,matthews_corrcoef\n",
    "        from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "        from skopt import BayesSearchCV\n",
    "        from skopt.space import Real, Categorical, Integer\n",
    "        from math import floor, ceil\n",
    "        for i in range(0,len(samples)):\n",
    "            model_inst=model_rep()\n",
    "            print(\"Sampling technique: \",samples[i][2], \"\\n \")\n",
    "#           IMPORTANT when inputting default hyperparameters:\n",
    "#           wrap non int or float types with Categorical() function\n",
    "#           NEVER input int or float types with only one parameter option... this will not work with bayes search (works fine with randomized search)\n",
    "            if(\"RandomForest\" in str(model_rep)):\n",
    "                tree_ensemble=True\n",
    "#               default hyperparameter testing range\n",
    "                bootstrap = Categorical([True, False])\n",
    "                n_estimators = [350,450]\n",
    "                criterion= Categorical(['gini','entropy'])\n",
    "                max_depth =np.arange(1,floor(len(X_train.columns)),1)\n",
    "                max_features = np.random.uniform(0.01,1,10000)\n",
    "                min_samples_split = np.random.uniform(0.01,1,10000)\n",
    "                min_samples_leaf = np.random.uniform(0.0001,0.5,10000)\n",
    "                class_weight = Categorical(['balanced','balanced_subsample',None])\n",
    "                n_jobs = [-1]\n",
    "                random_state = [self.random_state]\n",
    "#               input hyperparameters into dictionary\n",
    "                param_grid = dict(n_estimators=n_estimators,\n",
    "                                  bootstrap=bootstrap, \n",
    "                                  criterion=criterion,\n",
    "                                  min_samples_leaf=min_samples_leaf, \n",
    "                                  min_samples_split=min_samples_split,\n",
    "                                  max_features=max_features,\n",
    "                                  max_depth=max_depth,\n",
    "                                  class_weight=class_weight,\n",
    "                                  n_jobs=n_jobs,\n",
    "                                  random_state=random_state)\n",
    "\n",
    "            elif(\"XGB\" in str(model_rep)):\n",
    "                tree_ensemble=True\n",
    "    #               default hyperparameter testing range\n",
    "                booster = Categorical(['gbtree','gblinear','dart'])\n",
    "                n_estimators = [350,450]\n",
    "                learning_rate = np.random.uniform(0.000001,1,10000)\n",
    "                max_depth = np.arange(1,floor(len(X_train.columns)),1)\n",
    "                gamma = np.random.uniform(0,15,10000)\n",
    "                reg_alpha = np.random.uniform(0,1,10000)\n",
    "                reg_lambda = np.random.uniform(0,1,10000)\n",
    "                objective = Categorical(['reg:logistic'])\n",
    "                subsample = np.random.beta(2,5,10000)\n",
    "                colsample_bytree = np.random.beta(2,5,10000)\n",
    "                scale_pos_weight = np.random.uniform(0,20,10000)\n",
    "                min_child_weight = np.random.uniform(0,0.5*floor(len(X_train.columns)),10000)\n",
    "                max_delta_step = 0.2*floor(len(X_train.columns))*np.random.beta(2,5,10000)\n",
    "                n_jobs = [-1]\n",
    "                random_state = [self.random_state]\n",
    "    #               input hyperparameters into dictionary\n",
    "                param_grid = dict(n_estimators=n_estimators,\n",
    "                                  booster=booster,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  max_depth=max_depth,\n",
    "                                  gamma=gamma,\n",
    "                                  reg_alpha=reg_alpha,\n",
    "                                  reg_lambda=reg_lambda,\n",
    "                                  objective=objective,\n",
    "                                  subsample=subsample,\n",
    "                                  colsample_bytree=colsample_bytree,\n",
    "                                  scale_pos_weight=scale_pos_weight,\n",
    "                                  min_child_weight=min_child_weight,\n",
    "                                  max_delta_step=max_delta_step,\n",
    "                                  n_jobs=n_jobs,\n",
    "                                  random_state=random_state)          \n",
    "\n",
    "            elif(\"LogisticRegression\" in str(model_rep)):\n",
    "                tree_ensemble=False\n",
    "#               default hyperparameter testing range\n",
    "                penalty = Categorical(['l2'])\n",
    "                tol = np.random.uniform(0.00000001,0.01,10000)\n",
    "                C = np.random.uniform(0.000001,1,10000)\n",
    "                fit_intercept = Categorical([True,False])\n",
    "                intercept_scaling = np.random.uniform(0.01,10,10000)\n",
    "                class_weight = Categorical(['balanced',None])\n",
    "                solver = Categorical(['newton-cg', 'lbfgs','sag'])\n",
    "                max_iter = np.arange(100,1000,10)\n",
    "                n_jobs = [-1]\n",
    "                random_state = [self.random_state]\n",
    "#               input hyperparameters into dictionary\n",
    "                param_grid = dict(penalty=penalty,\n",
    "                                  tol=tol,\n",
    "                                  C=C,\n",
    "                                  fit_intercept=fit_intercept,\n",
    "                                  intercept_scaling=intercept_scaling,\n",
    "                                  class_weight=class_weight,\n",
    "                                  solver=solver,\n",
    "                                  max_iter=max_iter,\n",
    "                                  n_jobs=n_jobs,\n",
    "                                  random_state=random_state)\n",
    "\n",
    "            elif(\"Isolation\" in str(model_rep)):\n",
    "                tree_ensemble = True\n",
    "#               default hyperparameter testing range\n",
    "                bootstrap = Categorical([True, False])\n",
    "                n_estimators = [350,450]\n",
    "                max_samples= np.random.uniform(0.01,1,10000)\n",
    "                max_features = np.random.uniform(0.01,1,10000)\n",
    "                contamination = np.random.uniform(0,0.2,10000)\n",
    "                behaviour = Categorical(['new'])\n",
    "                n_jobs = [-1]\n",
    "                random_state = [self.random_state]\n",
    "#               input hyperparameters into dictionary\n",
    "                param_grid = dict(n_estimators=n_estimators,\n",
    "                                  bootstrap=bootstrap, \n",
    "                                  max_features=max_features,\n",
    "                                  contamination=contamination,\n",
    "                                  max_samples=max_samples,\n",
    "                                  behaviour=behaviour,\n",
    "                                  n_jobs=n_jobs,\n",
    "                                  random_state=random_state)\n",
    "\n",
    "            #elif(\"Bagging\" in str(model)):\n",
    "\n",
    "#           instantiate cv and grid\n",
    "            kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)\n",
    "            tuning_strategy=''.join(tuning_strategy.split()).lower()\n",
    "            if tuning_strategy=='bayes':\n",
    "                param_grid_cat = {k: v for k, v in param_grid.items() if isinstance(v,Categorical)==True or k=='n_jobs' or k=='random_state'}\n",
    "                param_grid_num = {k:(min(v),max(v)) for k,v in param_grid.items() if k not in param_grid_cat}\n",
    "                param_grid={**param_grid_num,**param_grid_cat}\n",
    "                import warnings\n",
    "                warnings.filterwarnings('ignore', message='The objective has been evaluated at this point before.')\n",
    "                grid_search = BayesSearchCV(model_inst, param_grid, scoring=tuning_metric, n_jobs=-1, cv=kfold, n_iter=floor(tuning_iter/len(samples)),verbose=0)\n",
    "            elif tuning_strategy=='randomized':\n",
    "                grid_search = RandomizedSearchCV(model_inst, param_grid, scoring=tuning_metric, n_jobs=-1, cv=kfold, n_iter=floor(tuning_iter/len(samples)),verbose=0)\n",
    "            print('Tuning...')\n",
    "            grid_results=grid_search.fit(samples[i][0],samples[i][1])\n",
    "            best_param[i]=grid_results.best_params_\n",
    "            print(\"Best: {} using {} \\n\".format(grid_results.best_score_, best_param[i]))\n",
    "            if(tree_ensemble==True):\n",
    "                best_param[i].update({'n_estimators': 800})\n",
    "            del grid_results\n",
    "#           Train model\n",
    "            model=model_rep\n",
    "            model=model(**best_param[i])\n",
    "            print(\"Fitting model...\")\n",
    "            model.fit(samples[i][0],samples[i][1])\n",
    "            preds = model.predict(X_test[samples[i][0].columns])\n",
    "            conf_mat = confusion_matrix(y_true=y_test, y_pred=preds)\n",
    "            accuracy = accuracy_score(y_test, preds)\n",
    "            ck = cohen_kappa_score(y_test,preds)\n",
    "            brier = brier_score_loss(y_test,preds)\n",
    "            auc = roc_auc_score(y_test, preds) \n",
    "            f1 = f1_score(y_test, preds)\n",
    "#               jaccard = jaccard_score(y_test, preds)\n",
    "            recall = recall_score(y_test, preds)\n",
    "            precision = precision_score(y_test, preds)\n",
    "            mcc = matthews_corrcoef(y_test, preds)\n",
    "            specificity=round(conf_mat[0][0] / (conf_mat[0][0]+conf_mat[0][1]),5)\n",
    "            neg_pred= round(conf_mat[0][0] / (conf_mat[0][0]+conf_mat[1][0]),5)\n",
    "            f2=round(5*((precision*recall)/((4*precision)+recall)),5)\n",
    "#               rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "            print(\"Accuracy:          \", accuracy)\n",
    "            print('Precision:         ', precision)\n",
    "            print('Recall:            ', recall)\n",
    "            print('Specificity:       ', specificity)\n",
    "            print('Neg Pred Val:      ', neg_pred)\n",
    "            print('Confusion Sum:     ', precision+recall+specificity+neg_pred)                \n",
    "            print(' ')\n",
    "            print('F1 score:          ', f1)\n",
    "            print('F2 score:          ', f2)\n",
    "            print('Cohen kappa score: ', ck)\n",
    "#               print(\"RMSE:              \", rmse)\n",
    "            print(' ')\n",
    "#               print(\"Jaccard score:     \", jaccard)\n",
    "            print(\"Brier score loss:  \", brier)\n",
    "            print('MCC:               ', mcc)\n",
    "            print(\"AUC:               \", auc, \"\\n \",\"\\n \",\"\\n \",\"\\n \",\"\\n \") \n",
    "            df=pd.DataFrame({\"Sampling\" : samples[i][2],\n",
    "                                           \"Accuracy\" : round(accuracy,5),\n",
    "                                           'Precision' : round(precision,5),\n",
    "                                           'Recall' : round(recall,5),\n",
    "                                           'Specificity' : round(specificity,5),\n",
    "                                           'Neg Pred Val' : round(neg_pred,5),\n",
    "                                           'Confusion Sum' : round(precision+recall+specificity+neg_pred,5),\n",
    "                                           'F1 score' : round(f1,5),\n",
    "                                           'F2 score' : round(f2,5),\n",
    "                                           'Cohen kappa score' : round(ck,5),\n",
    "#                                              \"RMSE\" : rmse,\n",
    "#                                              \"Jaccard score\" : jaccard,\n",
    "                                           \"Brier score loss\" : round(brier,5),\n",
    "                                           'MCC' : round(mcc,5),\n",
    "                                           \"AUC\" : round(auc,5)},index=[i])\n",
    "            df2 = pd.DataFrame({'Sampling': samples[i][2],\n",
    "                                            'AUC': round(auc,5),\n",
    "                                            'F1': round(f1,5),\n",
    "                                            'F2': round(f2,5),\n",
    "                                            'Cohen kappa' : round(ck,5),\n",
    "                                            'Brier': round(brier,5),\n",
    "                                            'MCC': round(mcc,5)},index=[i])\n",
    "            self.metrics=self.metrics.append(df)\n",
    "            radar_df=radar_df.append(df2)\n",
    "        import matplotlib.pyplot as plt\n",
    "        from math import pi\n",
    "        # ------- RADAR CHARTS PART 1: Create background\n",
    "        # number of variables\n",
    "        categories=list(radar_df)[1:]\n",
    "        N = len(categories)\n",
    "        # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "        angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "        angles += angles[:1]\n",
    "        # Initialise the spider plot\n",
    "        ax = plt.subplot(111, polar=True)\n",
    "        # If you want the first axis to be on top:\n",
    "        ax.set_theta_offset(pi / 2)\n",
    "        ax.set_theta_direction(-1)\n",
    "        # Draw one axe per variable + add labels labels yet\n",
    "        plt.xticks(angles[:-1], categories)\n",
    "        # Draw ylabels\n",
    "        ax.set_rlabel_position(0)\n",
    "        plt.yticks([0.2,0.4,0.6,0.8], [\"0.2\",\"0.4\",\"0.6\",\"0.8\"], color=\"blue\", size=7)\n",
    "        plt.ylim(0,1)\n",
    "        # ------- PART 2: Add plots\n",
    "        # Plot each individual = each line of the data\n",
    "        # Ind1\n",
    "        values=radar_df.loc[2].drop('Sampling').values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=\"ROS\")\n",
    "        ax.fill(angles, values, 'b', alpha=0.1)\n",
    "        # Ind2\n",
    "        values=radar_df.loc[3].drop('Sampling').values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=\"SMOTE\")\n",
    "        ax.fill(angles, values, 'r', alpha=0.1)\n",
    "        # Ind3\n",
    "        values=radar_df.loc[1].drop('Sampling').values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=\"RUS\")\n",
    "        ax.fill(angles, values, 'k', alpha=0.1)\n",
    "        # Ind4\n",
    "        values=radar_df.loc[0].drop('Sampling').values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=\"No Sampling\")\n",
    "        ax.fill(angles, values, 'green', alpha=0.1)\n",
    "        # Add legend\n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
