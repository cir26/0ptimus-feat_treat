{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import data which was subset using R\n",
    "df_y6= pd.read_csv('/home/cristianromero/Documents/Datasets/kantar_6_targets.csv')\n",
    "df_x6= pd.read_csv('/home/cristianromero/Documents/Datasets/kantar_6_features.csv')\n",
    "\n",
    "# remove extra id columns (relic of exporting data from R)\n",
    "df_y6=df_y6.iloc[:,1]\n",
    "df_x6=df_x6.iloc[:,1:len(df_x6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical column indices:  [1, 2, 4, 5, 27, 70, 86, 162, 164, 165, 166, 167, 172, 173] \n",
      " \n",
      " Class  Count\n",
      "     0   6640\n",
      "     1   2290 \n",
      " \n",
      "Base rate:  0.2564389697648376\n",
      "                                    column_name  percent_na\n",
      "10          hh_pcnt_pop_professional_occupation    0.011198\n",
      "5           hh_median_rent_as_pcnt_of_hh_income    0.022396\n",
      "11                              hh_pcnt_poverty    0.022396\n",
      "12                        hh_pcnt_self_employed    0.022396\n",
      "2                 hh_average_worker_travel_time    0.111982\n",
      "14                           hh_pcnt_unemployed    0.212766\n",
      "8                     hh_pcnt_housing_300k_plus    0.268757\n",
      "15                 hh_pcnt_workers_work_at_home    0.492721\n",
      "6                                    hh_ownrent    0.683091\n",
      "9              hh_pcnt_income_public_assistance    1.041433\n",
      "7                   hh_pcnt_hh_income_150k_plus    1.276596\n",
      "13  hh_pcnt_total_work_16_plus_use_public_trans    2.799552\n",
      "17                           hh_vehicles_per_hh    2.922732\n",
      "0                                           age    3.840985\n",
      "1                          hh_actual_hom_evalue   11.534155\n",
      "4                       hh_home_equity_estimate   15.072788\n",
      "16                    hh_revolver_to_transactor   25.095185\n",
      "18                    ind_number_of_trade_lines   78.230683\n",
      "3                                hh_boat_length   95.319149 \n",
      " \n",
      "Empty DataFrame\n",
      "Columns: [column_name, percent_na]\n",
      "Index: [] \n",
      " \n",
      "Empty DataFrame\n",
      "Columns: [column_name, percent_na]\n",
      "Index: [] \n",
      " \n"
     ]
    }
   ],
   "source": [
    "start=feat_treat(df_x6,df_y6,1)\n",
    "# impute\n",
    "start.check_na(50)\n",
    "start.handle_na('remove',start.na_col)\n",
    "start.check_na()\n",
    "start.handle_na('median',start.na_col)\n",
    "start.check_na()\n",
    "start.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copies of prepped data set\n",
    "import copy\n",
    "xgb_pca=copy.deepcopy(start)\n",
    "xgb_svd=copy.deepcopy(start)\n",
    "xgb_rfe=copy.deepcopy(start)\n",
    "xgb_rfe_pca=copy.deepcopy(start)\n",
    "xgb_rfe_svd=copy.deepcopy(start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform \n",
    "xgb_pca.pca(24)\n",
    "\n",
    "xgb_svd.svd(22)\n",
    "\n",
    "xgb_rfe.rfe(n=40)\n",
    "\n",
    "xgb_rfe_pca.rfe(n=40)\n",
    "xgb_rfe_pca.pca(8)\n",
    "\n",
    "xgb_rfe_svd.rfe(n=40)\n",
    "xgb_rfe_svd.svd(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model= XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator:  <class 'xgboost.sklearn.XGBClassifier'>\n",
      "Sampling technique:  None \n",
      " \n",
      "Tuning...\n",
      "Best: 0.6172209654907407 using {'booster': 'gblinear', 'colsample_bytree': 0.5378340890463584, 'gamma': 0.6764989070470256, 'learning_rate': 0.7195417225059336, 'max_delta_step': 1.1632916406300742, 'max_depth': 2, 'min_child_weight': 5.59262713250664, 'n_estimators': 350, 'n_jobs': -1, 'objective': 'reg:logistic', 'random_state': 1, 'reg_alpha': 0.5035627728301827, 'reg_lambda': 0.41038734642173114, 'scale_pos_weight': 14.108305930028838, 'subsample': 0.38614666681730253, 'verbosity': 0} \n",
      "\n",
      "Fitting model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristianromero/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:543: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/home/cristianromero/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:442: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:           0.270996640537514\n",
      "Precision:          0.270996640537514\n",
      "Recall:             1.0\n",
      "Specificity:        0.0\n",
      "Neg Pred Val:       nan\n",
      "Confusion Sum:      nan\n",
      " \n",
      "F1 score:           0.426431718061674\n",
      "F2 score:           0.65019\n",
      "Cohen kappa score:  0.0\n",
      " \n",
      "Brier score loss:   0.729003359462486\n",
      "MCC:                0.0\n",
      "AUC:                0.5 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      " \n",
      "Sampling technique:  Random under sampling \n",
      " \n",
      "Tuning...\n",
      "Best: 0.6165640641227215 using {'booster': 'gblinear', 'colsample_bytree': 0.08986970648667374, 'gamma': 10.632027941676125, 'learning_rate': 0.5140302455769745, 'max_delta_step': 0.13569359359457064, 'max_depth': 3, 'min_child_weight': 6.880626899439296, 'n_estimators': 350, 'n_jobs': -1, 'objective': 'reg:logistic', 'random_state': 1, 'reg_alpha': 0.28123924519269905, 'reg_lambda': 0.9606533205460521, 'scale_pos_weight': 3.4201857008948644, 'subsample': 0.3040068501863272, 'verbosity': 0} \n",
      "\n",
      "Fitting model...\n",
      "Accuracy:           0.2704367301231803\n",
      "Precision:          0.27058823529411763\n",
      "Recall:             0.9979338842975206\n",
      "Specificity:        0.0\n",
      "Neg Pred Val:       0.0\n",
      "Confusion Sum:      1.2685221195916383\n",
      " \n",
      "F1 score:           0.4257382106654914\n",
      "F2 score:           0.64902\n",
      "Cohen kappa score:  -0.0011202148885334218\n",
      " \n",
      "Brier score loss:   0.7295632698768197\n",
      "MCC:                -0.03882073029493376\n",
      "AUC:                0.4989669421487603 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      " \n",
      "Sampling technique:  Random over sampling \n",
      " \n",
      "Tuning...\n",
      "Best: 0.9338528582739305 using {'booster': 'gbtree', 'colsample_bytree': 0.19321916252092497, 'gamma': 1.163981932888415, 'learning_rate': 0.5881563173230763, 'max_delta_step': 0.8285959146756506, 'max_depth': 10, 'min_child_weight': 9.746650555523136, 'n_estimators': 350, 'n_jobs': -1, 'objective': 'reg:logistic', 'random_state': 1, 'reg_alpha': 0.9305076920264895, 'reg_lambda': 0.5908807050685602, 'scale_pos_weight': 17.7112245175857, 'subsample': 0.48886247633407676, 'verbosity': 0} \n",
      "\n",
      "Fitting model...\n",
      "Accuracy:           0.6349384098544233\n",
      "Precision:          0.3489208633093525\n",
      "Recall:             0.40082644628099173\n",
      "Specificity:        0.72197\n",
      "Neg Pred Val:       0.76423\n",
      "Confusion Sum:      2.235947309590344\n",
      " \n",
      "F1 score:           0.3730769230769231\n",
      "F2 score:           0.38925\n",
      "Cohen kappa score:  0.11731067772764758\n",
      " \n",
      "Brier score loss:   0.3650615901455767\n",
      "MCC:                0.11787198600212177\n",
      "AUC:                0.5613963260590826 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      " \n",
      "Sampling technique:  SMOTE \n",
      " \n",
      "Tuning...\n",
      "Best: 0.7671930962342125 using {'booster': 'gbtree', 'colsample_bytree': 0.21516251185484528, 'gamma': 7.209229809711553, 'learning_rate': 0.5438574685010498, 'max_delta_step': 3.2583242459328, 'max_depth': 21, 'min_child_weight': 3.227693931163581, 'n_estimators': 350, 'n_jobs': -1, 'objective': 'reg:logistic', 'random_state': 1, 'reg_alpha': 0.9442048748598857, 'reg_lambda': 0.42050225066736713, 'scale_pos_weight': 13.169802836503141, 'subsample': 0.411722039305684, 'verbosity': 0} \n",
      "\n",
      "Fitting model...\n",
      "Accuracy:           0.5582306830907054\n",
      "Precision:          0.30013106159895153\n",
      "Recall:             0.4731404958677686\n",
      "Specificity:        0.58986\n",
      "Neg Pred Val:       0.75073\n",
      "Confusion Sum:      2.1138615574667203\n",
      " \n",
      "F1 score:           0.36728147554129914\n",
      "F2 score:           0.42423\n",
      "Cohen kappa score:  0.053342899638442076\n",
      " \n",
      "Brier score loss:   0.4417693169092945\n",
      "MCC:                0.05660882313636084\n",
      "AUC:                0.5315011235099212 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      " \n",
      "Sampling technique:  SMOTE + TL \n",
      " \n",
      "Tuning...\n",
      "Best: 0.797506208102105 using {'booster': 'dart', 'colsample_bytree': 0.2458387629034395, 'gamma': 7.501215931375472, 'learning_rate': 0.5351739807960166, 'max_delta_step': 1.307689779695191, 'max_depth': 6, 'min_child_weight': 4.376897008778853, 'n_estimators': 350, 'n_jobs': -1, 'objective': 'reg:logistic', 'random_state': 1, 'reg_alpha': 0.816445100278545, 'reg_lambda': 0.10259090002527693, 'scale_pos_weight': 16.838893251158165, 'subsample': 0.49144080579867855, 'verbosity': 0} \n",
      "\n",
      "Fitting model...\n",
      "Accuracy:           0.5487122060470325\n",
      "Precision:          0.29620253164556964\n",
      "Recall:             0.4834710743801653\n",
      "Specificity:        0.57296\n",
      "Neg Pred Val:       0.749\n",
      "Confusion Sum:      2.101633606025735\n",
      " \n",
      "F1 score:           0.3673469387755102\n",
      "F2 score:           0.4292\n",
      "Cohen kappa score:  0.04708455466675132\n",
      " \n",
      "Brier score loss:   0.45128779395296753\n",
      "MCC:                0.05050556266300658\n",
      "AUC:                0.5282178720595143 \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      " \n",
      "Seconds elapsed:  591.9208497399231 per  50  iterations\n",
      "Minutes elapsed:  9.865347495665386 per  50  iterations\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "xgb_pca.sample_tune_test(model=model, tuning_iter=iterations)\n",
    "end=timer()\n",
    "print(\"Seconds elapsed: \",(end - start), \"per \",iterations,\" iterations\")\n",
    "print(\"Minutes elapsed: \",(end - start)/60, \"per \",iterations,\" iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "xgb_svd.sample_tune_test(model=model, tuning_iter=iterations)\n",
    "end=timer()\n",
    "print(\"Seconds elapsed: \",(end - start), \"per \",iterations,\" iterations\")\n",
    "print(\"Minutes elapsed: \",(end - start)/60, \"per \",iterations,\" iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "xgb_rfe.sample_tune_test(model=model, tuning_iter=iterations)\n",
    "end=timer()\n",
    "print(\"Seconds elapsed: \",(end - start), \"per \",iterations,\" iterations\")\n",
    "print(\"Minutes elapsed: \",(end - start)/60, \"per \",iterations,\" iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "xgb_rfe_pca.sample_tune_test(model=model, tuning_iter=iterations)\n",
    "end=timer()\n",
    "print(\"Seconds elapsed: \",(end - start), \"per \",iterations,\" iterations\")\n",
    "print(\"Minutes elapsed: \",(end - start)/60, \"per \",iterations,\" iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "xgb_rfe_svd.sample_tune_test(model=model, tuning_iter=iterations)\n",
    "end=timer()\n",
    "print(\"Seconds elapsed: \",(end - start), \"per \",iterations,\" iterations\")\n",
    "print(\"Minutes elapsed: \",(end - start)/60, \"per \",iterations,\" iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature treatment class\n",
    "# Use to optimize feature selection, sampling, and hyperparameter tuning for model testing\n",
    "# Stream-lines testing process and brings together packages like scikit-learn, skopt, pandas, numpy \n",
    "# create instances of feature treatments \n",
    "# returns performance metrics for treatments with visual\n",
    "\n",
    "# Run Time: \n",
    "# RandomForestClassifier = 4.5 iterations/min\n",
    "# XGBClassifier = 5.2 iterations/min\n",
    "\n",
    "# required arguments: \n",
    "# X = feature pandas dataframe long format\n",
    "# y = dependent variable pandas dataframe long format\n",
    "# random_state = seed for pseudo random number generator to be used throughout treatments\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class feat_treat:\n",
    "    def __init__(self,X,y,random_state):\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.random_state=random_state\n",
    "        self.encoded=False\n",
    "        self.na_col='update with check_na(percent_threshold)'\n",
    "        self.metrics='No metrics available'\n",
    "        \n",
    "        # Low variance filter \n",
    "        # check variance of each column\n",
    "        # remove column if variance is less than var_threshold\n",
    "        drop=[]\n",
    "        var_threshold = 0.004975\n",
    "        for col in self.X.columns:\n",
    "            if self.X[col].dtype!='object': \n",
    "                if np.var(self.X[col]) < var_threshold:\n",
    "                    drop.append(col)\n",
    "        self.X=self.X.drop(columns=drop)\n",
    "        \n",
    "        # GET TO KNOW YOUR DATA SET\n",
    "        # return dataframe indices for categorical datatypes\n",
    "        cat_col=[]\n",
    "        for i in self.X.columns:\n",
    "            if self.X[i].dtype == 'object':\n",
    "                cat_col.append(self.X.columns.get_loc(i))\n",
    "        print(\"categorical column indices: \",cat_col,\"\\n \")\n",
    "    \n",
    "        # return base rate\n",
    "        unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "        DV_classes_df = pd.DataFrame({'Class': unique_elements,\n",
    "                                      'Count': counts_elements})\n",
    "        print(DV_classes_df.to_string(index=False),\"\\n \")\n",
    "        print(\"Base rate: \",counts_elements[1]/sum(counts_elements))\n",
    "    \n",
    "    \n",
    "\n",
    "    def check_na(self,percent_threshold=0):\n",
    "        # check for missing values  \n",
    "        percent_missing = self.X.isnull().sum() * 100 / len(self.X)\n",
    "        column_name = [percent_missing.index[i] for i in range(0,len(self.X.columns)) if percent_missing[i]>0]\n",
    "        percent_na = [percent_missing[i] for i in range(0,len(self.X.columns)) if percent_missing[i]>0]\n",
    "        missing_value_df = pd.DataFrame({'column_name': column_name,\n",
    "                                         'percent_na': percent_na})\n",
    "        missing_value_df.sort_values('percent_na', inplace=True)\n",
    "        print(missing_value_df, \"\\n \")\n",
    "        self.na_col=column_name\n",
    "    \n",
    "    \n",
    "    \n",
    "    def handle_na(self,strategy,columns):\n",
    "        # handle missing values\n",
    "        # methods: mean, median, mode, value, remove\n",
    "        from scipy import stats\n",
    "        \n",
    "        if isinstance(strategy,int)==True or isinstance(strategy,float)==True:\n",
    "            for i in columns:\n",
    "                self.X[i]=self.X[i].fillna(strategy)\n",
    "        else:\n",
    "            strategy1=''.join(strategy.split()).lower()\n",
    "            if strategy1=='mean':\n",
    "                for i in columns:\n",
    "                    mean=self.X[i].mean()\n",
    "                    self.X[i].fillna(mean,inplace=True)\n",
    "            elif strategy1=='median':\n",
    "                for i in columns:\n",
    "                    median=self.X[i].median()\n",
    "                    self.X[i].fillna(median,inplace=True)                            \n",
    "            elif strategy1=='mode':\n",
    "                for i in columns:\n",
    "                    mode = self.X[i].mode()[0]\n",
    "                    self.X[i].fillna(mode,inplace=True)                      \n",
    "            elif strategy1=='remove':\n",
    "                self.X.drop(columns=self.na_col,inplace=True) \n",
    "            else:\n",
    "                for i in columns:\n",
    "                    self.X[i].fillna(strategy,inplace=True)                 \n",
    "         \n",
    "    \n",
    "    \n",
    "    def pcc_filter(self,k):\n",
    "        if isinstance(k,int)==True or isinstance(k,float)==True:\n",
    "            # check pearson coefficient for linear correlation between DV and each feature \n",
    "            # remove column if PCC is less than correlation limit\n",
    "  \n",
    "            df=pd.concat([self.y, self.X], axis=1, ignore_index=True)\n",
    "#           create pearson correlation coefficient matrix\n",
    "            cor = df.corr()\n",
    "            corr_limit=k\n",
    "            cor_target = abs(cor.iloc[:,0])\n",
    "            relevant_features = cor_target[cor_target>corr_limit]\n",
    "            relevant_features=relevant_features.iloc[1:]\n",
    "            rel_col =[]\n",
    "            for i in relevant_features.index:\n",
    "                rel_col.append(i-1) \n",
    "#           drop all columns except relevant ones\n",
    "            self.X=self.X.iloc[:,rel_col]\n",
    "#           check which columns are kept \n",
    "            keep_col=self.X.columns\n",
    "            print(keep_col)\n",
    "            print(len(keep_col))            \n",
    "#       section to include pcc optimization loop\n",
    "        else:\n",
    "            k = ''.join(k.split()).lower()\n",
    "            k = k[:3]\n",
    "            if k == 'opt':\n",
    "                pass\n",
    "\n",
    "    \n",
    "    \n",
    "    def collinear(self,k):\n",
    "#       check pearson coefficient for linear correlation between features (collinearity)\n",
    "#       remove if PCC is greater than collinear limit\n",
    "#       recreate pearson correlation matrix\n",
    "        cor = self.X.corr()\n",
    "        feat_targets=[]\n",
    "        feat_remove=[]\n",
    "        ignore=[]\n",
    "#       set correlation limit\n",
    "        colinear_corr_limit=k\n",
    "#       check for collinearity\n",
    "        for i in self.X.columns:\n",
    "            cor_target = abs(cor.loc[:,i])\n",
    "            feat_targets.append((i,cor_target[cor_target>colinear_corr_limit]))  \n",
    "        # remove collinear features\n",
    "        for i in feat_targets:\n",
    "            ignore.append(i[0])\n",
    "            for j in i[1].index:\n",
    "                if j not in ignore:\n",
    "                    feat_remove.append(j)\n",
    "\n",
    "        feat_remove=set(feat_remove)\n",
    "        self.X=self.X.drop(columns=feat_remove)\n",
    "        # check which columns are kept \n",
    "        keep_col=self.X.columns\n",
    "        print(\"columns remaining: \",keep_col)\n",
    "        print(len(keep_col), \" columns\")\n",
    "                      \n",
    "    \n",
    "    def encode(self, strategy=None):\n",
    "        if(strategy=='dummy'):\n",
    "            self.X=pd.get_dummies(self.X)\n",
    "            self.encoded=True    \n",
    "        else:\n",
    "            from sklearn import preprocessing\n",
    "            cat_col=[]\n",
    "            for i in self.X.columns:\n",
    "                if self.X[i].dtype == 'object':\n",
    "                    cat_col.append(i)\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            for col in cat_col:\n",
    "                self.X[col]=le.fit_transform(self.X[col])            \n",
    "            self.encoded=True    \n",
    "        \n",
    "    \n",
    "    \n",
    "    def rfe(self,n,rfe_model=None):\n",
    "        if isinstance(n,int)==True or isinstance(n,float)==True:\n",
    "            if rfe_model==None:\n",
    "                from sklearn.linear_model import LogisticRegression\n",
    "                rfe_model=LogisticRegression(solver='lbfgs')\n",
    "                from sklearn.feature_selection import RFE\n",
    "                selector = RFE(estimator=rfe_model, n_features_to_select = n).fit(self.X,self.y)\n",
    "                keep = [i for i in range(0,len(selector.support_)) if selector.support_[i]==True]\n",
    "                self.X=self.X.iloc[:,keep]\n",
    "            else:\n",
    "                from sklearn.feature_selection import RFE\n",
    "                selector = RFE(estimator=rfe_model, n_features_to_select = n).fit(self.X,self.y)\n",
    "                keep = [i for i in range(0,len(selector.support_)) if selector.support_[i]==True]\n",
    "                self.X=self.iloc[:,keep]\n",
    "#       section to include optimization loop\n",
    "        else:\n",
    "            n = ''.join(n.split()).lower()\n",
    "            n = n[:3]\n",
    "            if n == 'opt':\n",
    "                pass                \n",
    "\n",
    "            \n",
    "\n",
    "    def skb(self,k,score_func=None):\n",
    "        if self.encoded == True:\n",
    "            if isinstance(k,int)==True or isinstance(k,float)==True:\n",
    "                if score_func==None:\n",
    "                    from sklearn.feature_selection import SelectKBest, chi2\n",
    "                    skb =SelectKBest(score_func=chi2, k=k).fit(self.X,self.y)\n",
    "                    self.X = self.X.iloc[:,skb.get_support(indices=True)]\n",
    "                else:\n",
    "                    from sklearn.feature_selection import SelectKBest, score_func\n",
    "                    skb = SelectKBest(score_func=score_func, k=k).fit(self.X,self.y)\n",
    "                    self.X = self.X.iloc[:,skb.get_support(indices=True)]\n",
    "\n",
    "#           section to include optimization loop\n",
    "            else:\n",
    "                n = ''.join(k.split()).lower()\n",
    "                n = n[:3]\n",
    "                if n == 'opt':\n",
    "                    pass              \n",
    "        else:\n",
    "            print('Please encode feature set first')\n",
    "            \n",
    "            \n",
    "            \n",
    "    def svd(self,n):\n",
    "        if isinstance(n,int)==True or isinstance(n,float)==True:\n",
    "            col = self.X.columns\n",
    "            from sklearn.decomposition import TruncatedSVD\n",
    "            svd = TruncatedSVD(n_components=n, n_iter=5, random_state=self.random_state).fit(self.X)\n",
    "            self.X = pd.DataFrame(svd.transform(self.X), columns=['SV %i' % i for i in range(n)], index=self.X.index)  \n",
    "#       section to include optimization loop\n",
    "        else:\n",
    "            n = ''.join(n.split()).lower()\n",
    "            n = n[:3]\n",
    "            if n == 'opt':\n",
    "                pass             \n",
    "            \n",
    "\n",
    "            \n",
    "    def pca(self,n):\n",
    "        if isinstance(n,int)==True or isinstance(n,float)==True:\n",
    "            from sklearn.decomposition import PCA\n",
    "            pca = PCA(n_components=n, random_state=self.random_state).fit(self.X) \n",
    "            self.X = pd.DataFrame(pca.transform(self.X), columns=['PCA %i' % i for i in range(n)], index=self.X.index) \n",
    "#       section to include optimization loop\n",
    "        else:\n",
    "            n = ''.join(n.split()).lower()\n",
    "            n = n[:3]\n",
    "            if n == 'opt':\n",
    "                pass             \n",
    "\n",
    "    \n",
    "#   def sample(strategy):\n",
    "    def sample_tune_test(self,model,tuning_iter,tuning_strategy='randomized',tuning_metric='roc_auc',test_size=0.2):\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=test_size, random_state=self.random_state)\n",
    "#       function to automatically sample, HP tune, then test with best parameters\n",
    "        col = X_train.columns\n",
    "        from imblearn.under_sampling import RandomUnderSampler\n",
    "        rus = RandomUnderSampler()\n",
    "        X_rus, y_rus = rus.fit_sample(X_train, y_train)\n",
    "        X_rus = pd.DataFrame(X_rus, columns = col)\n",
    "        from imblearn.over_sampling import RandomOverSampler\n",
    "        ros = RandomOverSampler()\n",
    "        X_ros, y_ros = ros.fit_sample(X_train, y_train)\n",
    "        X_ros = pd.DataFrame(X_ros, columns = col)\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        smote = SMOTE(ratio='minority')\n",
    "        X_sm, y_sm = smote.fit_sample(X_train, y_train)\n",
    "        X_sm = pd.DataFrame(X_sm, columns = col)\n",
    "        from imblearn.combine import SMOTETomek\n",
    "        smt = SMOTETomek(ratio='auto')\n",
    "        X_smt, y_smt = smt.fit_sample(X_train, y_train)\n",
    "        X_smt = pd.DataFrame(X_smt, columns = col)\n",
    "        samples = [(X_train,y_train,\"None\"),\n",
    "                  (X_rus,y_rus,\"Random under sampling\"),\n",
    "                  (X_ros,y_ros,\"Random over sampling\"),\n",
    "                  (X_sm,y_sm,\"SMOTE\"),\n",
    "                  (X_smt,y_smt,\"SMOTE + TL\")] \n",
    "#       tune and test\n",
    "        best_param=[0]*len(samples)\n",
    "        self.metrics=pd.DataFrame(columns=[\"Sampling\",\n",
    "                                           \"Accuracy\",\n",
    "                                           'Precision',\n",
    "                                           'Recall',\n",
    "                                           'Specificity',\n",
    "                                           'Neg Pred Val',\n",
    "                                           'Confusion Sum',\n",
    "                                           'F1 score', \n",
    "                                           'F2 score',\n",
    "                                           'Cohen kappa score',\n",
    "#                                              \"RMSE\" : rmse,\n",
    "#                                              \"Jaccard score\" : jaccard,\n",
    "                                           \"Brier score loss\",\n",
    "                                           'MCC',\n",
    "                                           \"AUC\"])\n",
    "        radar_df = pd.DataFrame(columns=['Sampling',\n",
    "                                            'AUC',\n",
    "                                            'F1', \n",
    "                                            'F2',\n",
    "                                            'Cohen kappa',\n",
    "                                            'Brier',\n",
    "                                            'MCC'])\n",
    "        model_rep=model\n",
    "        print(\"Estimator: \",model_rep)\n",
    "        from sklearn.metrics import cohen_kappa_score,accuracy_score,roc_auc_score,brier_score_loss,confusion_matrix,f1_score,recall_score,precision_score,matthews_corrcoef\n",
    "        from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "        from skopt import BayesSearchCV\n",
    "        from skopt.space import Real, Categorical, Integer\n",
    "        from math import floor, ceil\n",
    "        for i in range(0,len(samples)):\n",
    "            model_inst=model_rep()\n",
    "            print(\"Sampling technique: \",samples[i][2], \"\\n \")\n",
    "#           IMPORTANT when inputting default hyperparameters:\n",
    "#           wrap non int or float types with Categorical() function\n",
    "#           NEVER input int or float types with only one parameter option... this will not work with bayes search (works fine with randomized search)\n",
    "            if(\"RandomForest\" in str(model_rep)):\n",
    "                tree_ensemble=True\n",
    "#               default hyperparameter testing range\n",
    "                bootstrap = Categorical([True, False])\n",
    "                n_estimators = [350,450]\n",
    "                criterion= Categorical(['gini','entropy'])\n",
    "                max_depth =np.arange(1,floor(len(X_train.columns)),1)\n",
    "                max_features = np.random.uniform(0.01,1,10000)\n",
    "                min_samples_split = np.random.uniform(0.01,1,10000)\n",
    "                min_samples_leaf = np.random.uniform(0.0001,0.5,10000)\n",
    "                class_weight = Categorical(['balanced','balanced_subsample',None])\n",
    "                n_jobs = [-1]\n",
    "                random_state = [self.random_state]\n",
    "#               input hyperparameters into dictionary\n",
    "                param_grid = dict(n_estimators=n_estimators,\n",
    "                                  bootstrap=bootstrap, \n",
    "                                  criterion=criterion,\n",
    "                                  min_samples_leaf=min_samples_leaf, \n",
    "                                  min_samples_split=min_samples_split,\n",
    "                                  max_features=max_features,\n",
    "                                  max_depth=max_depth,\n",
    "                                  class_weight=class_weight,\n",
    "                                  n_jobs=n_jobs,\n",
    "                                  random_state=random_state)\n",
    "\n",
    "            elif(\"XGB\" in str(model_rep)):\n",
    "                tree_ensemble=True\n",
    "    #               default hyperparameter testing range\n",
    "                booster = Categorical(['gbtree','gblinear','dart'])\n",
    "                n_estimators = [350,450]\n",
    "                learning_rate = np.random.uniform(0.000001,1,10000)\n",
    "                max_depth = np.arange(1,floor(len(X_train.columns)),1)\n",
    "                gamma = np.random.uniform(0,15,10000)\n",
    "                reg_alpha = np.random.uniform(0,1,10000)\n",
    "                reg_lambda = np.random.uniform(0,1,10000)\n",
    "                objective = Categorical(['reg:logistic'])\n",
    "                subsample = np.random.beta(2,5,10000)\n",
    "                colsample_bytree = np.random.beta(2,5,10000)\n",
    "                scale_pos_weight = np.random.uniform(0,20,10000)\n",
    "                min_child_weight = np.random.uniform(0,0.5*floor(len(X_train.columns)),10000)\n",
    "                max_delta_step = 0.2*floor(len(X_train.columns))*np.random.beta(2,5,10000)\n",
    "                n_jobs = [-1]\n",
    "                random_state = [self.random_state]\n",
    "    #               input hyperparameters into dictionary\n",
    "                param_grid = dict(n_estimators=n_estimators,\n",
    "                                  booster=booster,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  max_depth=max_depth,\n",
    "                                  gamma=gamma,\n",
    "                                  reg_alpha=reg_alpha,\n",
    "                                  reg_lambda=reg_lambda,\n",
    "                                  objective=objective,\n",
    "                                  subsample=subsample,\n",
    "                                  colsample_bytree=colsample_bytree,\n",
    "                                  scale_pos_weight=scale_pos_weight,\n",
    "                                  min_child_weight=min_child_weight,\n",
    "                                  max_delta_step=max_delta_step,\n",
    "                                  n_jobs=n_jobs,\n",
    "                                  random_state=random_state)          \n",
    "\n",
    "            elif(\"LogisticRegression\" in str(model_rep)):\n",
    "                tree_ensemble=False\n",
    "#               default hyperparameter testing range\n",
    "                penalty = Categorical(['l2'])\n",
    "                tol = np.random.uniform(0.00000001,0.01,10000)\n",
    "                C = np.random.uniform(0.000001,1,10000)\n",
    "                fit_intercept = Categorical([True,False])\n",
    "                intercept_scaling = np.random.uniform(0.01,10,10000)\n",
    "                class_weight = Categorical(['balanced',None])\n",
    "                solver = Categorical(['newton-cg', 'lbfgs','sag'])\n",
    "                max_iter = np.arange(100,1000,10)\n",
    "                n_jobs = [-1]\n",
    "                random_state = [self.random_state]\n",
    "#               input hyperparameters into dictionary\n",
    "                param_grid = dict(penalty=penalty,\n",
    "                                  tol=tol,\n",
    "                                  C=C,\n",
    "                                  fit_intercept=fit_intercept,\n",
    "                                  intercept_scaling=intercept_scaling,\n",
    "                                  class_weight=class_weight,\n",
    "                                  solver=solver,\n",
    "                                  max_iter=max_iter,\n",
    "                                  n_jobs=n_jobs,\n",
    "                                  random_state=random_state)\n",
    "\n",
    "            elif(\"Isolation\" in str(model_rep)):\n",
    "                tree_ensemble = True\n",
    "#               default hyperparameter testing range\n",
    "                bootstrap = Categorical([True, False])\n",
    "                n_estimators = [350,450]\n",
    "                max_samples= np.random.uniform(0.01,1,10000)\n",
    "                max_features = np.random.uniform(0.01,1,10000)\n",
    "                contamination = np.random.uniform(0,0.2,10000)\n",
    "                behaviour = Categorical(['new'])\n",
    "                n_jobs = [-1]\n",
    "                random_state = [self.random_state]\n",
    "#               input hyperparameters into dictionary\n",
    "                param_grid = dict(n_estimators=n_estimators,\n",
    "                                  bootstrap=bootstrap, \n",
    "                                  max_features=max_features,\n",
    "                                  contamination=contamination,\n",
    "                                  max_samples=max_samples,\n",
    "                                  behaviour=behaviour,\n",
    "                                  n_jobs=n_jobs,\n",
    "                                  random_state=random_state)\n",
    "\n",
    "            #elif(\"Bagging\" in str(model)):\n",
    "\n",
    "#           instantiate cv and grid\n",
    "            kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)\n",
    "            tuning_strategy=''.join(tuning_strategy.split()).lower()\n",
    "            if tuning_strategy=='bayes':\n",
    "                param_grid_cat = {k: v for k, v in param_grid.items() if isinstance(v,Categorical)==True or k=='n_jobs' or k=='random_state'}\n",
    "                param_grid_num = {k:(min(v),max(v)) for k,v in param_grid.items() if k not in param_grid_cat}\n",
    "                param_grid={**param_grid_num,**param_grid_cat}\n",
    "                import warnings\n",
    "                warnings.filterwarnings('ignore', message='The objective has been evaluated at this point before.')\n",
    "                grid_search = BayesSearchCV(model_inst, param_grid, scoring=tuning_metric, n_jobs=-1, cv=kfold, n_iter=floor(tuning_iter/len(samples)),verbose=0)\n",
    "            elif tuning_strategy=='randomized':\n",
    "                grid_search = RandomizedSearchCV(model_inst, param_grid, scoring=tuning_metric, n_jobs=-1, cv=kfold, n_iter=floor(tuning_iter/len(samples)),verbose=0)\n",
    "            print('Tuning...')\n",
    "            grid_results=grid_search.fit(samples[i][0],samples[i][1])\n",
    "            best_param[i]=grid_results.best_params_\n",
    "            print(\"Best: {} using {} \\n\".format(grid_results.best_score_, best_param[i]))\n",
    "            if(tree_ensemble==True):\n",
    "                best_param[i].update({'n_estimators': 800})\n",
    "            del grid_results\n",
    "#           Train model\n",
    "            model=model_rep\n",
    "            model=model(**best_param[i])\n",
    "            print(\"Fitting model...\")\n",
    "            model.fit(samples[i][0],samples[i][1])\n",
    "            preds = model.predict(X_test[samples[i][0].columns])\n",
    "            conf_mat = confusion_matrix(y_true=y_test, y_pred=preds)\n",
    "            accuracy = accuracy_score(y_test, preds)\n",
    "            ck = cohen_kappa_score(y_test,preds)\n",
    "            brier = brier_score_loss(y_test,preds)\n",
    "            auc = roc_auc_score(y_test, preds) \n",
    "            f1 = f1_score(y_test, preds)\n",
    "#               jaccard = jaccard_score(y_test, preds)\n",
    "            recall = recall_score(y_test, preds)\n",
    "            precision = precision_score(y_test, preds)\n",
    "            mcc = matthews_corrcoef(y_test, preds)\n",
    "            specificity=round(conf_mat[0][0] / (conf_mat[0][0]+conf_mat[0][1]),5)\n",
    "            neg_pred= round(conf_mat[0][0] / (conf_mat[0][0]+conf_mat[1][0]),5)\n",
    "            f2=round(5*((precision*recall)/((4*precision)+recall)),5)\n",
    "#               rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "            print(\"Accuracy:          \", accuracy)\n",
    "            print('Precision:         ', precision)\n",
    "            print('Recall:            ', recall)\n",
    "            print('Specificity:       ', specificity)\n",
    "            print('Neg Pred Val:      ', neg_pred)\n",
    "            print('Confusion Sum:     ', precision+recall+specificity+neg_pred)                \n",
    "            print(' ')\n",
    "            print('F1 score:          ', f1)\n",
    "            print('F2 score:          ', f2)\n",
    "            print('Cohen kappa score: ', ck)\n",
    "#               print(\"RMSE:              \", rmse)\n",
    "            print(' ')\n",
    "#               print(\"Jaccard score:     \", jaccard)\n",
    "            print(\"Brier score loss:  \", brier)\n",
    "            print('MCC:               ', mcc)\n",
    "            print(\"AUC:               \", auc, \"\\n \",\"\\n \",\"\\n \",\"\\n \",\"\\n \") \n",
    "            df=pd.DataFrame({\"Sampling\" : samples[i][2],\n",
    "                                           \"Accuracy\" : round(accuracy,5),\n",
    "                                           'Precision' : round(precision,5),\n",
    "                                           'Recall' : round(recall,5),\n",
    "                                           'Specificity' : round(specificity,5),\n",
    "                                           'Neg Pred Val' : round(neg_pred,5),\n",
    "                                           'Confusion Sum' : round(precision+recall+specificity+neg_pred,5),\n",
    "                                           'F1 score' : round(f1,5),\n",
    "                                           'F2 score' : round(f2,5),\n",
    "                                           'Cohen kappa score' : round(ck,5),\n",
    "#                                              \"RMSE\" : rmse,\n",
    "#                                              \"Jaccard score\" : jaccard,\n",
    "                                           \"Brier score loss\" : round(brier,5),\n",
    "                                           'MCC' : round(mcc,5),\n",
    "                                           \"AUC\" : round(auc,5)},index=[i])\n",
    "            df2 = pd.DataFrame({'Sampling': samples[i][2],\n",
    "                                            'AUC': round(auc,5),\n",
    "                                            'F1': round(f1,5),\n",
    "                                            'F2': round(f2,5),\n",
    "                                            'Cohen kappa' : round(ck,5),\n",
    "                                            'Brier': round(brier,5),\n",
    "                                            'MCC': round(mcc,5)},index=[i])\n",
    "            self.metrics=self.metrics.append(df)\n",
    "            radar_df=radar_df.append(df2)\n",
    "        import matplotlib.pyplot as plt\n",
    "        from math import pi\n",
    "        # ------- RADAR CHARTS PART 1: Create background\n",
    "        # number of variables\n",
    "        categories=list(radar_df)[1:]\n",
    "        N = len(categories)\n",
    "        # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "        angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "        angles += angles[:1]\n",
    "        # Initialise the spider plot\n",
    "        ax = plt.subplot(111, polar=True)\n",
    "        # If you want the first axis to be on top:\n",
    "        ax.set_theta_offset(pi / 2)\n",
    "        ax.set_theta_direction(-1)\n",
    "        # Draw one axe per variable + add labels labels yet\n",
    "        plt.xticks(angles[:-1], categories)\n",
    "        # Draw ylabels\n",
    "        ax.set_rlabel_position(0)\n",
    "        plt.yticks([0.2,0.4,0.6,0.8], [\"0.2\",\"0.4\",\"0.6\",\"0.8\"], color=\"blue\", size=7)\n",
    "        plt.ylim(0,1)\n",
    "        # ------- PART 2: Add plots\n",
    "        # Plot each individual = each line of the data\n",
    "        # Ind1\n",
    "        values=radar_df.loc[2].drop('Sampling').values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=\"ROS\")\n",
    "        ax.fill(angles, values, 'b', alpha=0.1)\n",
    "        # Ind2\n",
    "        values=radar_df.loc[3].drop('Sampling').values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=\"SMOTE\")\n",
    "        ax.fill(angles, values, 'r', alpha=0.1)\n",
    "        # Ind3\n",
    "        values=radar_df.loc[1].drop('Sampling').values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=\"RUS\")\n",
    "        ax.fill(angles, values, 'k', alpha=0.1)\n",
    "        # Ind4\n",
    "        values=radar_df.loc[0].drop('Sampling').values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=\"No Sampling\")\n",
    "        ax.fill(angles, values, 'green', alpha=0.1)\n",
    "        # Add legend\n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
