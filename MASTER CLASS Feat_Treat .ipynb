{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feat_treat() class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature treatment class\n",
    "# Use to optimize feature selection, sampling, and hyperparameter tuning for model testing\n",
    "# Stream-lines testing process and brings together packages like scikit-learn, skopt, pandas, numpy \n",
    "# create instances of feature treatments \n",
    "# returns performance metrics for treatments with visual\n",
    "\n",
    "# Run Time: \n",
    "# RandomForestClassifier = 4.5 iterations/min\n",
    "# XGBClassifier = 5.2 iterations/min\n",
    "\n",
    "# required arguments: \n",
    "# X = feature pandas dataframe long format\n",
    "# y = dependent variable pandas dataframe long format\n",
    "# random_state = seed for pseudo random number generator to be used throughout treatments\n",
    "\n",
    "# main dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from math import floor, ceil, pi\n",
    "# scikit tools\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score,accuracy_score,roc_auc_score,brier_score_loss,confusion_matrix,f1_score,recall_score,precision_score,matthews_corrcoef\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "# sampling tools\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "class feat_treat:\n",
    "    def __init__(self,X,y,random_state):\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.random_state=random_state\n",
    "        self.encoded=False\n",
    "        self.na_col='update with check_na(percent_threshold)'\n",
    "        self.metrics='No metrics available'\n",
    "        self.model=\"No model available\"\n",
    "        \n",
    "        # Low variance filter \n",
    "        # check variance of each column\n",
    "        # remove column if variance is less than var_threshold\n",
    "        drop=[]\n",
    "        var_threshold = 0.004975\n",
    "        for col in self.X.columns:\n",
    "            if self.X[col].dtype!='object': \n",
    "                if np.var(self.X[col]) < var_threshold:\n",
    "                    drop.append(col)\n",
    "        self.X=self.X.drop(columns=drop)\n",
    "        \n",
    "        # GET TO KNOW YOUR DATA SET\n",
    "        # return dataframe indices for categorical datatypes\n",
    "        cat_col=[self.X.columns.get_loc(i) for i in self.X.columns if self.X[i].dtype == 'object']\n",
    "        print(\"categorical column indices: \",cat_col,\"\\n \")\n",
    "    \n",
    "        # return base rate\n",
    "        unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
    "        DV_classes_df = pd.DataFrame({'Class': unique_elements,\n",
    "                                      'Count': counts_elements})\n",
    "        print(DV_classes_df.to_string(index=False),\"\\n \")\n",
    "        print(\"Base rate: \",counts_elements[1]/sum(counts_elements))\n",
    "    \n",
    "\n",
    "    def check_na(self,percent_threshold=0):\n",
    "        # check for missing values  \n",
    "        percent_missing = self.X.isnull().sum() * 100 / len(self.X)\n",
    "        column_name = [percent_missing.index[i] for i in range(0,len(self.X.columns)) if percent_missing[i]>percent_threshold]\n",
    "        percent_na = [percent_missing[i] for i in range(0,len(self.X.columns)) if percent_missing[i]>percent_threshold]\n",
    "        missing_value_df = pd.DataFrame({'column_name': column_name,\n",
    "                                         'percent_na': percent_na})\n",
    "        missing_value_df.sort_values('percent_na', inplace=True)\n",
    "        print(missing_value_df, \"\\n \")\n",
    "        self.na_col=column_name\n",
    "    \n",
    "    \n",
    "    \n",
    "    def handle_na(self,strategy,columns):\n",
    "        # handle missing values\n",
    "        # methods: mean, median, mode, value, remove\n",
    "\n",
    "        if isinstance(strategy,int)==True or isinstance(strategy,float)==True:\n",
    "            for i in columns:\n",
    "                self.X[i]=self.X[i].fillna(strategy)\n",
    "        else:\n",
    "            strategy1=''.join(strategy.split()).lower()\n",
    "            if strategy1=='mean':\n",
    "                for i in columns:\n",
    "                    mean=self.X[i].mean()\n",
    "                    self.X[i].fillna(mean,inplace=True)\n",
    "            elif strategy1=='median':\n",
    "                for i in columns:\n",
    "                    median=self.X[i].median()\n",
    "                    self.X[i].fillna(median,inplace=True)                            \n",
    "            elif strategy1=='mode':\n",
    "                for i in columns:\n",
    "                    mode = self.X[i].mode()[0]\n",
    "                    self.X[i].fillna(mode,inplace=True)                      \n",
    "            elif strategy1=='remove':\n",
    "                self.X.drop(columns=self.na_col,inplace=True) \n",
    "            elif strategy1=='random':\n",
    "                for i in columns:\n",
    "                    uniques = np.unique(self.X[i])\n",
    "                    uniques = uniques[~np.isnan(uniques)]\n",
    "                    for j in range(0,len(self.X[i])):\n",
    "                        if np.isnan(self.X.loc[self.X.index[j],i])==True:\n",
    "                            self.X.loc[self.X.index[j],i] = np.random.choice(uniques)\n",
    "#             elif strategy1=='mice': # mice will perform impute on all columns\n",
    "#                 from impyute.imputation.cs import mice\n",
    "#                 col=self.X.columns\n",
    "#                 X_imp=mice(self.X)\n",
    "#                 self.X=pd.DataFrame(X_imp, columns=col)\n",
    "            else:\n",
    "                for i in columns:\n",
    "                    self.X[i].fillna(strategy,inplace=True)                 \n",
    "         \n",
    "    \n",
    "    \n",
    "    def pcc_filter(self,k):\n",
    "        if isinstance(k,int)==True or isinstance(k,float)==True:\n",
    "            # check pearson coefficient for linear correlation between DV and each feature \n",
    "            # remove column if PCC is less than correlation limit\n",
    "  \n",
    "            df=pd.concat([self.y, self.X], axis=1, ignore_index=True)\n",
    "#           create pearson correlation coefficient matrix\n",
    "            cor = df.corr()\n",
    "            corr_limit=k\n",
    "            cor_target = abs(cor.iloc[:,0])\n",
    "            relevant_features = cor_target[cor_target>corr_limit]\n",
    "            relevant_features=relevant_features.iloc[1:]\n",
    "            rel_col =[]\n",
    "            for i in relevant_features.index:\n",
    "                rel_col.append(i-1) \n",
    "#           drop all columns except relevant ones\n",
    "            self.X=self.X.iloc[:,rel_col]\n",
    "#           check which columns are kept \n",
    "            keep_col=self.X.columns\n",
    "            print(keep_col)\n",
    "            print(len(keep_col))            \n",
    "#       section to include pcc optimization loop\n",
    "        else:\n",
    "            k = ''.join(k.split()).lower()\n",
    "            k = k[:3]\n",
    "            if k == 'opt':\n",
    "                pass\n",
    "\n",
    "    \n",
    "    \n",
    "    def collinear(self,k):\n",
    "#       check pearson coefficient for linear correlation between features (collinearity)\n",
    "#       remove if PCC is greater than collinear limit\n",
    "#       recreate pearson correlation matrix\n",
    "        cor = self.X.corr()\n",
    "        feat_targets=[]\n",
    "        feat_remove=[]\n",
    "        ignore=[]\n",
    "#       set correlation limit\n",
    "        colinear_corr_limit=k\n",
    "#       check for collinearity\n",
    "        for i in self.X.columns:\n",
    "            cor_target = abs(cor.loc[:,i])\n",
    "            feat_targets.append((i,cor_target[cor_target>colinear_corr_limit]))  \n",
    "        # remove collinear features\n",
    "        for i in feat_targets:\n",
    "            ignore.append(i[0])\n",
    "            for j in i[1].index:\n",
    "                if j not in ignore:\n",
    "                    feat_remove.append(j)\n",
    "\n",
    "        feat_remove=set(feat_remove)\n",
    "        self.X=self.X.drop(columns=feat_remove)\n",
    "        # check which columns are kept \n",
    "        keep_col=self.X.columns\n",
    "        print(\"columns remaining: \",keep_col)\n",
    "        print(len(keep_col), \" columns\")\n",
    "                      \n",
    "    \n",
    "    def encode(self, strategy=None):\n",
    "        if(strategy=='dummy'):\n",
    "            self.X=pd.get_dummies(self.X)\n",
    "            self.encoded=True    \n",
    "        else:\n",
    "            cat_col=[i for i in self.X.columns if self.X[i].dtype == 'object']\n",
    "            if len(cat_col)>0:\n",
    "                le = preprocessing.LabelEncoder()\n",
    "                for col in cat_col:\n",
    "                    self.X[col]=le.fit_transform(self.X[col])            \n",
    "                self.encoded=True\n",
    "            else:\n",
    "                print(\"No categorical variables in data set\")\n",
    "        \n",
    "\n",
    "    def rfe(self,n=None,cum=None,rfe_model=None):\n",
    "        if isinstance(n,int)==True and cum==None:\n",
    "            if rfe_model==None:\n",
    "                from sklearn.linear_model import LogisticRegression\n",
    "                rfe_model=LogisticRegression(solver='lbfgs')\n",
    "                kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)\n",
    "                selector = RFECV(estimator=rfe_model, min_features_to_select = n, cv=kfold, n_jobs=-1).fit(self.X,self.y)\n",
    "                keep = [i for i in range(0,len(selector.support_)) if selector.support_[i]==True]\n",
    "                self.X=self.X.iloc[:,keep]\n",
    "            else:\n",
    "                kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)\n",
    "                selector = RFECV(estimator=rfe_model, min_features_to_select = n, cv=kfold, n_jobs=-1).fit(self.X,self.y)\n",
    "                keep = [i for i in range(0,len(selector.support_)) if selector.support_[i]==True]\n",
    "                self.X=self.iloc[:,keep]\n",
    "        elif isinstance(cum,float)==True and n==None:\n",
    "            # cumulative feature importance\n",
    "            pass\n",
    "#       section to include optimization loop\n",
    "        else:\n",
    "            n = ''.join(n.split()).lower()\n",
    "            n = n[:3]\n",
    "            if n == 'opt':\n",
    "                pass                \n",
    "\n",
    "            \n",
    "\n",
    "    def skb(self,k,score_func=None):\n",
    "        cat_col=[self.X.columns.get_loc(i) for i in self.X.columns if self.X[i].dtype == 'object']\n",
    "        if self.encoded == True or len(cat_col)==0:\n",
    "            if isinstance(k,int)==True or isinstance(k,float)==True:\n",
    "                if score_func==None:\n",
    "                    from sklearn.feature_selection import chi2\n",
    "                    skb =SelectKBest(score_func=chi2, k=k).fit(self.X,self.y)\n",
    "                    self.X = self.X.iloc[:,skb.get_support(indices=True)]\n",
    "                else:\n",
    "                    from sklearn.feature_selection import score_func\n",
    "                    skb = SelectKBest(score_func=score_func, k=k).fit(self.X,self.y)\n",
    "                    self.X = self.X.iloc[:,skb.get_support(indices=True)]\n",
    "#           section to include optimization loop\n",
    "            else:\n",
    "                n = ''.join(k.split()).lower()\n",
    "                n = n[:3]\n",
    "                if n == 'opt':\n",
    "                    pass              \n",
    "        else:\n",
    "            print('Please encode feature set first')\n",
    "            \n",
    "            \n",
    "            \n",
    "    def svd(self,n=None, var=None):\n",
    "        if var==None and isinstance(n,int)==True:\n",
    "            col = self.X.columns\n",
    "            svd = TruncatedSVD(n_components=n, n_iter=5, random_state=self.random_state).fit(self.X)\n",
    "            self.X = pd.DataFrame(svd.transform(self.X), columns=['SV %i' % i for i in range(n)], index=self.X.index)\n",
    "        elif n==None and isinstance(var,float)==True:\n",
    "            n=len(self.X.columns)\n",
    "            exp_var = 0\n",
    "            svd = TruncatedSVD(n_components=n, n_iter=5, random_state=self.random_state).fit(self.X) \n",
    "            j=0\n",
    "            exp_var=0\n",
    "            while exp_var<var and j<n:\n",
    "                exp_var = exp_var + svd.explained_variance_ratio_[j]\n",
    "                j=j+1\n",
    "            svd = TruncatedSVD(n_components=j, n_iter=5, random_state=self.random_state).fit(self.X) \n",
    "            self.X = pd.DataFrame(svd.transform(self.X), columns=['SV %i' % i for i in range(n)], index=self.X.index)\n",
    "#       section to include optimization loop\n",
    "        else:\n",
    "            n = ''.join(n.split()).lower()\n",
    "            n = n[:3]\n",
    "            if n == 'opt':\n",
    "                pass             \n",
    "            \n",
    "\n",
    "            \n",
    "    def pca(self,n=None,var=None):\n",
    "        if var==None and isinstance(n,int)==True:\n",
    "            pca = PCA(n_components=n, random_state=self.random_state).fit(self.X) \n",
    "            self.X = pd.DataFrame(pca.transform(self.X), columns=['PCA %i' % i for i in range(n)], index=self.X.index) \n",
    "        elif n==None and isinstance(var,float)==True:\n",
    "            n=len(self.X.columns)\n",
    "            exp_var = 0\n",
    "            pca = PCA(n_components=n, random_state=self.random_state).fit(self.X) \n",
    "            j=0\n",
    "            exp_var=0\n",
    "            while exp_var<var and j<n:\n",
    "                exp_var = exp_var + pca.explained_variance_ratio_[j]\n",
    "                j=j+1\n",
    "            pca = PCA(n_components=j, random_state=self.random_state).fit(self.X) \n",
    "            self.X = pd.DataFrame(pca.transform(self.X), columns=['PCA %i' % i for i in range(n)], index=self.X.index) \n",
    "#       section to include optimization loop\n",
    "        else:\n",
    "            n = ''.join(n.split()).lower()\n",
    "            n = n[:3]\n",
    "            if n == 'opt':\n",
    "                pass             \n",
    "\n",
    "    \n",
    "#   def sample(strategy):\n",
    "    def sample_tune_test(self,model,tuning_iter,tuning_strategy='randomized',tuning_metric='roc_auc',test_size=0.2):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=test_size, random_state=self.random_state)\n",
    "#       function to automatically sample, HP tune, then test with best parameters\n",
    "        col = X_train.columns\n",
    "        \n",
    "        rus = RandomUnderSampler()\n",
    "        X_rus, y_rus = rus.fit_sample(X_train, y_train)\n",
    "        X_rus = pd.DataFrame(X_rus, columns = col)\n",
    "\n",
    "        ros = RandomOverSampler()\n",
    "        X_ros, y_ros = ros.fit_sample(X_train, y_train)\n",
    "        X_ros = pd.DataFrame(X_ros, columns = col)\n",
    "        \n",
    "        smote = SMOTE(ratio='minority')\n",
    "        X_sm, y_sm = smote.fit_sample(X_train, y_train)\n",
    "        X_sm = pd.DataFrame(X_sm, columns = col)\n",
    "\n",
    "        smt = SMOTETomek(ratio='auto')\n",
    "        X_smt, y_smt = smt.fit_sample(X_train, y_train)\n",
    "        X_smt = pd.DataFrame(X_smt, columns = col)\n",
    "        \n",
    "        samples = ((X_train,y_train,\"None\"),\n",
    "                  (X_rus,y_rus,\"Random under sampling\"),\n",
    "                  (X_ros,y_ros,\"Random over sampling\"),\n",
    "                  (X_sm,y_sm,\"SMOTE\"),\n",
    "                  (X_smt,y_smt,\"SMOTE + TL\")) \n",
    "        N = len(samples)\n",
    "#       tune and test\n",
    "        best_param=[0]*N\n",
    "        self.metrics=pd.DataFrame(columns=[\"Sampling\",\n",
    "                                           \"Accuracy\",\n",
    "                                           'Precision',\n",
    "                                           'Recall',\n",
    "                                           'Specificity',\n",
    "                                           'Neg Pred Val',\n",
    "                                           'Confusion Sum',\n",
    "                                           'F1 score', \n",
    "                                           'F2 score',\n",
    "                                           'F3 score',\n",
    "                                           'Cohen kappa score',\n",
    "#                                              \"RMSE\" : rmse,\n",
    "#                                              \"Jaccard score\" : jaccard,\n",
    "                                           \"Brier score loss\",\n",
    "                                           'MCC',\n",
    "                                           \"AUC\"])\n",
    "        radar_df = pd.DataFrame(columns=['Sampling',\n",
    "                                            'F1', \n",
    "                                            'F2',\n",
    "                                            'F3',\n",
    "                                            'Cohen kappa',\n",
    "                                            'MCC',\n",
    "                                            'AUC'])\n",
    "        model_rep=model\n",
    "        print(\"Estimator: \",model_rep)\n",
    "        \n",
    "        for i in range(0,N):\n",
    "            model_inst=model_rep()\n",
    "            print(\"Sampling technique: \",samples[i][2], \"\\n \")\n",
    "#           IMPORTANT when inputting default hyperparameters:\n",
    "#           wrap non int or float types with Categorical() function\n",
    "#           NEVER input int or float types with only one parameter option... this will not work with bayes search (works fine with randomized search)\n",
    "            if(\"RandomForest\" in str(model_rep)):\n",
    "                tree_ensemble=True\n",
    "#               default hyperparameter testing range\n",
    "                bootstrap = Categorical([True, False])\n",
    "                n_estimators = [350,450]\n",
    "                criterion= Categorical(['gini','entropy'])\n",
    "                max_depth =np.arange(1,floor(len(X_train.columns)),1)\n",
    "                max_features = np.random.uniform(0.01,1,10000)\n",
    "                min_samples_split = np.random.uniform(0.01,1,10000)\n",
    "                min_samples_leaf = np.random.uniform(0.0001,0.5,10000)\n",
    "                class_weight = Categorical(['balanced','balanced_subsample',None])\n",
    "                n_jobs = [-1]\n",
    "                random_state = [self.random_state]\n",
    "#               input hyperparameters into dictionary\n",
    "                param_grid = dict(n_estimators=n_estimators,\n",
    "                                  bootstrap=bootstrap, \n",
    "                                  criterion=criterion,\n",
    "                                  min_samples_leaf=min_samples_leaf, \n",
    "                                  min_samples_split=min_samples_split,\n",
    "                                  max_features=max_features,\n",
    "                                  max_depth=max_depth,\n",
    "                                  class_weight=class_weight,\n",
    "                                  n_jobs=n_jobs,\n",
    "                                  random_state=random_state)\n",
    "\n",
    "            elif(\"XGB\" in str(model_rep)):\n",
    "                tree_ensemble=True\n",
    "    #               default hyperparameter testing range\n",
    "                booster = Categorical(['gbtree','gblinear','dart'])\n",
    "                n_estimators = [350,450]\n",
    "                learning_rate = np.random.uniform(0.000001,1,10000)\n",
    "                max_depth = np.arange(1,floor(len(X_train.columns)),1)\n",
    "                gamma = np.random.uniform(0,15,10000)\n",
    "                reg_alpha = np.random.uniform(0,1,10000)\n",
    "                reg_lambda = np.random.uniform(0,1,10000)\n",
    "                objective = Categorical(['reg:logistic'])\n",
    "                subsample = np.random.beta(2,5,10000)\n",
    "                colsample_bytree = np.random.beta(2,5,10000)\n",
    "                scale_pos_weight = np.random.uniform(0,20,10000)\n",
    "                min_child_weight = np.random.uniform(0,0.5*floor(len(X_train.columns)),10000)\n",
    "                max_delta_step = 0.2*floor(len(X_train.columns))*np.random.beta(2,5,10000)\n",
    "                n_jobs = [-1]\n",
    "                random_state = [self.random_state]\n",
    "    #               input hyperparameters into dictionary\n",
    "                param_grid = dict(n_estimators=n_estimators,\n",
    "                                  booster=booster,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  max_depth=max_depth,\n",
    "                                  gamma=gamma,\n",
    "                                  reg_alpha=reg_alpha,\n",
    "                                  reg_lambda=reg_lambda,\n",
    "                                  objective=objective,\n",
    "                                  subsample=subsample,\n",
    "                                  colsample_bytree=colsample_bytree,\n",
    "                                  scale_pos_weight=scale_pos_weight,\n",
    "                                  min_child_weight=min_child_weight,\n",
    "                                  max_delta_step=max_delta_step,\n",
    "                                  n_jobs=n_jobs,\n",
    "                                  random_state=random_state)          \n",
    "\n",
    "            elif(\"LogisticRegression\" in str(model_rep)):\n",
    "                tree_ensemble=False\n",
    "#               default hyperparameter testing range\n",
    "                penalty = Categorical(['l2'])\n",
    "                tol = np.random.uniform(0.00000001,0.01,10000)\n",
    "                C = np.random.uniform(0.000001,1,10000)\n",
    "                fit_intercept = Categorical([True,False])\n",
    "                intercept_scaling = np.random.uniform(0.01,10,10000)\n",
    "                class_weight = Categorical(['balanced',None])\n",
    "                solver = Categorical(['newton-cg', 'lbfgs','sag'])\n",
    "                max_iter = np.arange(100,1000,10)\n",
    "                n_jobs = [-1]\n",
    "                random_state = [self.random_state]\n",
    "#               input hyperparameters into dictionary\n",
    "                param_grid = dict(penalty=penalty,\n",
    "                                  tol=tol,\n",
    "                                  C=C,\n",
    "                                  fit_intercept=fit_intercept,\n",
    "                                  intercept_scaling=intercept_scaling,\n",
    "                                  class_weight=class_weight,\n",
    "                                  solver=solver,\n",
    "                                  max_iter=max_iter,\n",
    "                                  n_jobs=n_jobs,\n",
    "                                  random_state=random_state)\n",
    "\n",
    "            elif(\"Isolation\" in str(model_rep)):\n",
    "                tree_ensemble = True\n",
    "#               default hyperparameter testing range\n",
    "                bootstrap = Categorical([True, False])\n",
    "                n_estimators = [350,450]\n",
    "                max_samples= np.random.uniform(0.01,1,10000)\n",
    "                max_features = np.random.uniform(0.01,1,10000)\n",
    "                contamination = np.random.uniform(0,0.2,10000)\n",
    "                behaviour = Categorical(['new'])\n",
    "                n_jobs = [-1]\n",
    "                random_state = [self.random_state]\n",
    "#               input hyperparameters into dictionary\n",
    "                param_grid = dict(n_estimators=n_estimators,\n",
    "                                  bootstrap=bootstrap, \n",
    "                                  max_features=max_features,\n",
    "                                  contamination=contamination,\n",
    "                                  max_samples=max_samples,\n",
    "                                  behaviour=behaviour,\n",
    "                                  n_jobs=n_jobs,\n",
    "                                  random_state=random_state)\n",
    "\n",
    "            #elif(\"Bagging\" in str(model)):\n",
    "\n",
    "#           instantiate cv and grid\n",
    "            kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)\n",
    "            tuning_strategy=''.join(tuning_strategy.split()).lower()\n",
    "            if tuning_strategy=='bayes':\n",
    "                # unpack parameters\n",
    "                param_grid_cat = {k: v for k, v in param_grid.items() if isinstance(v,Categorical)==True or k=='n_jobs' or k=='random_state'}\n",
    "                param_grid_num = {k:(min(v),max(v)) for k,v in param_grid.items() if k not in param_grid_cat}\n",
    "                param_grid={**param_grid_num,**param_grid_cat}\n",
    "                import warnings\n",
    "                warnings.filterwarnings('ignore', message='The objective has been evaluated at this point before.')\n",
    "                grid_search = BayesSearchCV(model_inst, param_grid, scoring=tuning_metric, n_jobs=-1, n_points=4, cv=kfold, n_iter=floor(tuning_iter/N),verbose=0)\n",
    "            elif tuning_strategy=='randomized':\n",
    "                grid_search = RandomizedSearchCV(model_inst, param_grid, scoring=tuning_metric, n_jobs=-1, cv=kfold, n_iter=floor(tuning_iter/N),verbose=0)\n",
    "            print('Tuning...')\n",
    "            grid_results=grid_search.fit(samples[i][0],samples[i][1])\n",
    "            best_param[i]=grid_results.best_params_\n",
    "            print(\"Best {}: {} using {} \\n\".format(tuning_metric, grid_results.best_score_, best_param[i]))\n",
    "            if(tree_ensemble==True):\n",
    "                best_param[i].update({'n_estimators': 800})\n",
    "            del grid_results\n",
    "#           Train model\n",
    "            model=model_rep\n",
    "            model=model(**best_param[i])\n",
    "            print(\"Fitting model...\")\n",
    "            model.fit(samples[i][0],samples[i][1])\n",
    "            preds = model.predict(X_test[samples[i][0].columns])\n",
    "            conf_mat = confusion_matrix(y_true=y_test, y_pred=preds)\n",
    "            accuracy = accuracy_score(y_test, preds)\n",
    "            ck = cohen_kappa_score(y_test,preds)\n",
    "            brier = brier_score_loss(y_test,preds)\n",
    "            auc = roc_auc_score(y_test, preds) \n",
    "            f1 = f1_score(y_test, preds)\n",
    "#               jaccard = jaccard_score(y_test, preds)\n",
    "            recall = recall_score(y_test, preds)\n",
    "            precision = precision_score(y_test, preds)\n",
    "            mcc = matthews_corrcoef(y_test, preds)\n",
    "            specificity=round(conf_mat[0][0] / (conf_mat[0][0]+conf_mat[0][1]),5)\n",
    "            neg_pred= round(conf_mat[0][0] / (conf_mat[0][0]+conf_mat[1][0]),5)\n",
    "            f2=round(5*((precision*recall)/((4*precision)+recall)),5)\n",
    "            f3=round(2*((specificity*neg_pred)/(specificity+neg_pred)),5)\n",
    "#               rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "            print(\"Accuracy:          \", accuracy)\n",
    "            print('Precision:         ', precision)\n",
    "            print('Recall:            ', recall)\n",
    "            print('Specificity:       ', specificity)\n",
    "            print('Neg Pred Val:      ', neg_pred)\n",
    "            print('Confusion Sum:     ', precision+recall+specificity+neg_pred)                \n",
    "            print(' ')\n",
    "            print('F1 score:          ', f1)\n",
    "            print('F2 score:          ', f2)\n",
    "            print('F3 score:          ', f3)\n",
    "            print('Cohen kappa score: ', ck)\n",
    "#               print(\"RMSE:              \", rmse)\n",
    "            print(' ')\n",
    "#               print(\"Jaccard score:     \", jaccard)\n",
    "            print(\"Brier score loss:  \", brier)\n",
    "            print('MCC:               ', mcc)\n",
    "            print(\"AUC:               \", auc, \"\\n \",\"\\n \",\"\\n \",\"\\n \",\"\\n \") \n",
    "            df=pd.DataFrame({\"Sampling\" : samples[i][2],\n",
    "                                           \"Accuracy\" : round(accuracy,5),\n",
    "                                           'Precision' : round(precision,5),\n",
    "                                           'Recall' : round(recall,5),\n",
    "                                           'Specificity' : round(specificity,5),\n",
    "                                           'Neg Pred Val' : round(neg_pred,5),\n",
    "                                           'Confusion Sum' : round(precision+recall+specificity+neg_pred,5),\n",
    "                                           'F1 score' : round(f1,5),\n",
    "                                           'F2 score' : round(f2,5),\n",
    "                                           'F3 score' : round(f3,5),\n",
    "                                           'Cohen kappa score' : round(ck,5),\n",
    "#                                              \"RMSE\" : rmse,\n",
    "#                                              \"Jaccard score\" : jaccard,\n",
    "                                           \"Brier score loss\" : round(brier,5),\n",
    "                                           'MCC' : round(mcc,5),\n",
    "                                           \"AUC\" : round(auc,5)},index=[i])\n",
    "            df2 = pd.DataFrame({'Sampling': samples[i][2],\n",
    "                                            'F1': round(f1,5),\n",
    "                                            'F2': round(f2,5),\n",
    "                                            'F3': round(f3,5),\n",
    "                                            'Cohen kappa' : round(ck,5),\n",
    "                                            'MCC': round(mcc,5),\n",
    "                                            'AUC': round(auc,5)},index=[i])\n",
    "            self.metrics=self.metrics.append(df)\n",
    "            radar_df=radar_df.append(df2)\n",
    "            radar_df=radar_df.fillna(0)\n",
    "            \n",
    "        # ------- RADAR CHARTS PART 1: Create background\n",
    "        # number of variables\n",
    "        categories=list(radar_df)[1:]\n",
    "        N = len(categories)\n",
    "        # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "        angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "        angles += angles[:1]\n",
    "        # Initialise the spider plot\n",
    "        ax = plt.subplot(111, polar=True)\n",
    "        # If you want the first axis to be on top:\n",
    "        ax.set_theta_offset(pi / 2)\n",
    "        ax.set_theta_direction(-1)\n",
    "        # Draw one axe per variable + add labels labels yet\n",
    "        plt.xticks(angles[:-1], categories)\n",
    "        # Draw ylabels\n",
    "        ax.set_rlabel_position(0)\n",
    "        plt.yticks([0.2,0.4,0.6,0.8], [\"0.2\",\"0.4\",\"0.6\",\"0.8\"], color=\"blue\", size=7)\n",
    "        plt.ylim(0,1)\n",
    "        # ------- PART 2: Add plots\n",
    "        # Plot each individual = each line of the data\n",
    "        # Ind1\n",
    "        values=radar_df.loc[2].drop('Sampling').values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        values = [abs(number) for number in values]\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=\"ROS\")\n",
    "        ax.fill(angles, values, 'b', alpha=0.1)\n",
    "        # Ind2\n",
    "        # if statement to display smote or smote+TL based on AUC score\n",
    "        if radar_df.iloc[3,6] >= radar_df.iloc[4,6]:\n",
    "            values=radar_df.loc[3].drop('Sampling').values.flatten().tolist()\n",
    "            values += values[:1]\n",
    "            values = [abs(number) for number in values]\n",
    "            ax.plot(angles, values, linewidth=1, linestyle='solid', label=\"SMOTE\")\n",
    "        else:\n",
    "            values=radar_df.loc[4].drop('Sampling').values.flatten().tolist()\n",
    "            values += values[:1]\n",
    "            values = [abs(number) for number in values]\n",
    "            ax.plot(angles, values, linewidth=1, linestyle='solid', label=\"SMOTE + TL\")\n",
    "        ax.fill(angles, values, 'r', alpha=0.1)\n",
    "        # Ind3\n",
    "        values=radar_df.loc[1].drop('Sampling').values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        values = [abs(number) for number in values]\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=\"RUS\")\n",
    "        ax.fill(angles, values, 'k', alpha=0.1)\n",
    "        # Ind4\n",
    "        values=radar_df.loc[0].drop('Sampling').values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        values = [abs(number) for number in values]\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=\"No Sampling\")\n",
    "        ax.fill(angles, values, 'green', alpha=0.1)\n",
    "        # Add legend\n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
